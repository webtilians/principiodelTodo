#!/usr/bin/env python3
"""
ğŸ” DIAGNÃ“STICO: Â¿Por quÃ© Attention Diversity = 0.5?
====================================================
"""

import torch
import sys
sys.path.insert(0, 'src')

# Cargar modelo
print("="*70)
print("ğŸ” DIAGNÃ“STICO ATTENTION DIVERSITY")
print("="*70)

# Cargar el modelo
checkpoint = torch.load('models/infinito_gpt2_spanish_phi.pt', map_location='cpu', weights_only=False)
print(f"âœ… Checkpoint cargado")

# Recrear el modelo para inspecciÃ³n
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from peft import get_peft_model, LoraConfig, TaskType

print(f"\nğŸ“¦ Cargando GPT-2 Spanish...")
tokenizer = GPT2Tokenizer.from_pretrained('datificate/gpt2-small-spanish', use_safetensors=True)

# Probar con attn_implementation="eager" para forzar atenciÃ³n tradicional
print("   â†’ Probando con attn_implementation='eager' para obtener attention weights...")
try:
    model = GPT2LMHeadModel.from_pretrained(
        'datificate/gpt2-small-spanish', 
        use_safetensors=True,
        attn_implementation="eager"  # Forzar atenciÃ³n tradicional
    )
    print("   âœ… Usando atenciÃ³n 'eager'")
except:
    model = GPT2LMHeadModel.from_pretrained('datificate/gpt2-small-spanish', use_safetensors=True)
    print("   âš ï¸ Fallback a atenciÃ³n default")

# Test de atenciÃ³n
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)
model.eval()

print(f"\nğŸ”¬ Ejecutando forward pass con output_attentions=True...")

test_text = "La inteligencia artificial es una tecnologÃ­a que"
inputs = tokenizer(test_text, return_tensors='pt').to(device)

with torch.no_grad():
    outputs = model(
        input_ids=inputs.input_ids,
        output_hidden_states=True,
        output_attentions=True,
        return_dict=True
    )

print(f"\nğŸ“Š ANÃLISIS DE OUTPUTS:")
print(f"   hidden_states: {len(outputs.hidden_states)} capas")
print(f"   attentions: {outputs.attentions}")  # â† Â¿Es None?

if outputs.attentions is not None:
    print(f"\n   Tipo: {type(outputs.attentions)}")
    print(f"   NÃºmero de capas: {len(outputs.attentions)}")
    
    last_attn = outputs.attentions[-1]
    print(f"\n   Ãšltima capa de atenciÃ³n:")
    print(f"   - Shape: {last_attn.shape}")  # [B, heads, T, T]
    print(f"   - Min: {last_attn.min().item():.6f}")
    print(f"   - Max: {last_attn.max().item():.6f}")
    print(f"   - Mean: {last_attn.mean().item():.6f}")
    
    # Calcular diversity manualmente
    print(f"\nğŸ§® CALCULANDO ATTENTION DIVERSITY MANUALMENTE:")
    
    # attention_weights: [B, heads, T, T]
    attn_flat = last_attn.mean(dim=1)  # [B, T, T]
    print(f"   attn_flat shape: {attn_flat.shape}")
    print(f"   attn_flat sum per row: {attn_flat.sum(dim=-1)}")  # DeberÃ­a ser ~1
    
    # EntropÃ­a de Shannon
    entropy = -torch.sum(attn_flat * torch.log(attn_flat + 1e-10), dim=-1)
    print(f"   entropy shape: {entropy.shape}")
    print(f"   entropy values: {entropy}")
    
    max_entropy = torch.log(torch.tensor(attn_flat.size(-1), dtype=torch.float, device=attn_flat.device))
    print(f"   max_entropy: {max_entropy.item():.4f}")
    
    # Normalizar
    diversity = entropy.mean(dim=-1) / max_entropy
    print(f"\n   ğŸ¯ ATTENTION DIVERSITY: {diversity.item():.4f}")
    
    if abs(diversity.item() - 0.5) < 0.01:
        print("\n   âš ï¸ Â¡ES ~0.5! Investigando por quÃ©...")
        
        # Â¿Es porque la atenciÃ³n es uniforme?
        seq_len = attn_flat.size(-1)
        uniform_entropy = torch.log(torch.tensor(seq_len, dtype=torch.float))
        print(f"\n   Si atenciÃ³n fuera uniforme:")
        print(f"   - EntropÃ­a uniforme: {uniform_entropy.item():.4f}")
        print(f"   - Diversity serÃ­a: {uniform_entropy.item() / max_entropy.item():.4f}")
        
        # Â¿Es causal mask el problema?
        print(f"\n   DistribuciÃ³n de atenciÃ³n (primer token):")
        print(f"   {attn_flat[0, 0, :].tolist()}")
        
        print(f"\n   DistribuciÃ³n de atenciÃ³n (Ãºltimo token):")
        print(f"   {attn_flat[0, -1, :].tolist()}")
        
else:
    print("\n   âŒ attentions is None!")
    print("   â†’ Este es el problema. GPT-2 no estÃ¡ devolviendo atenciones.")

print("\n" + "="*70)
print("ğŸ” DIAGNÃ“STICO COMPLETADO")
print("="*70)
