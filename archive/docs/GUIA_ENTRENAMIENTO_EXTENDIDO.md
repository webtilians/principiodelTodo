# üéì Gu√≠a de Entrenamiento Extendido

Esta gu√≠a explica las 3 formas de entrenar m√°s el modelo para mejorar la calidad del texto.

---

## üìã Resumen de Opciones

| Opci√≥n | Qu√© entrena | Tiempo estimado | Mejora esperada | Comando |
|--------|-------------|-----------------|-----------------|---------|
| **1. Continuar RL** | Agente RL desde 30K | 2-4 horas | +10-20% calidad decisiones | `continue_training_rl.py` |
| **2. Entrenar base** | Modelo INFINITO base | 4-8 horas | +30-50% calidad texto | `train_base_more.py` |
| **3. Nuevo RL completo** | Todo desde cero | 6-12 horas | Control total | `train_phi_text_scheduler.py` |

---

## üîÑ OPCI√ìN 1: Continuar RL desde 30K (M√°s r√°pido)

**¬øCu√°ndo usar?**
- Quieres mejorar las decisiones TEXT/PHI/MIXED
- El texto generado es aceptable pero las decisiones no son √≥ptimas
- Quieres resultados r√°pidos (2-4 horas)

### Comando b√°sico:
```bash
python continue_training_rl.py --timesteps 50000
```

### Comando con m√°s control:
```bash
python continue_training_rl.py \
  --checkpoint outputs/rl_phi_text_scheduler/checkpoints/ppo_infinito_scheduler_30000_steps.zip \
  --timesteps 100000 \
  --output outputs/rl_continued_100k \
  --save-freq 5000 \
  --eval-freq 5000
```

### Par√°metros:
- `--timesteps`: Pasos adicionales a entrenar (50K-200K recomendado)
- `--checkpoint`: Checkpoint base (default: 30K √≥ptimo)
- `--output`: Directorio de salida
- `--save-freq`: Cada cu√°ntos pasos guardar checkpoint
- `--eval-freq`: Cada cu√°ntos pasos evaluar

### Ventajas:
‚úÖ M√°s r√°pido (contin√∫a desde donde qued√≥)  
‚úÖ No pierde el progreso del 30K  
‚úÖ Mejora decisiones adaptativas  

### Desventajas:
‚ö†Ô∏è No mejora la calidad base del texto  
‚ö†Ô∏è Puede sobre-especializarse si entrenas demasiado  

---

## üìö OPCI√ìN 2: Entrenar Modelo Base (Mejor calidad)

**¬øCu√°ndo usar?**
- El texto tiene demasiada repetici√≥n
- Quieres mejorar la coherencia y fluidez
- No te importa entrenar 4-8 horas

### Comando b√°sico:
```bash
python train_base_more.py --epochs 20
```

### Comando optimizado (recomendado):
```bash
python train_base_more.py \
  --epochs 30 \
  --batch-size 16 \
  --lr 2e-4 \
  --lambda-phi 0.6 \
  --lora-r 8 \
  --patience 5
```

### Par√°metros importantes:
- `--epochs`: N√∫mero de √©pocas (20-50 recomendado)
  - 20 √©pocas: ~4 horas, mejora moderada
  - 30 √©pocas: ~6 horas, mejora significativa
  - 50 √©pocas: ~10 horas, mejora m√°xima
  
- `--lambda-phi`: Balance entre texto y PHI
  - 0.3: Prioriza calidad de texto
  - 0.6: Balance √≥ptimo (recomendado)
  - 1.0: Prioriza PHI (puede sacrificar fluidez)

- `--lora-r`: Capacidad de adaptaci√≥n
  - 4: M√≠nimo, r√°pido
  - 8: √ìptimo (recomendado)
  - 16: M√°ximo, m√°s lento

### Para continuar desde checkpoint:
```bash
python train_base_more.py \
  --epochs 20 \
  --checkpoint models/checkpoints/infinito_phase2_best.pt
```

### Ventajas:
‚úÖ Mejora dr√°sticamente la calidad del texto  
‚úÖ Reduce repeticiones  
‚úÖ Texto m√°s coherente y natural  

### Desventajas:
‚ö†Ô∏è Tarda m√°s tiempo  
‚ö†Ô∏è Requiere re-entrenar RL despu√©s  

---

## üöÄ OPCI√ìN 3: Entrenamiento RL Completo Nuevo

**¬øCu√°ndo usar?**
- Quieres cambiar hiperpar√°metros del RL
- El modelo actual no converge bien
- Quieres experimentar con configuraciones

### Comando b√°sico:
```bash
python experiments/train_phi_text_scheduler.py --timesteps 100000
```

### Comando con configuraci√≥n personalizada:
```bash
python experiments/train_phi_text_scheduler.py \
  --timesteps 150000 \
  --inner-steps 5 \
  --max-steps 50 \
  --batch-size 4 \
  --lr 3e-4 \
  --save-freq 5000 \
  --eval-freq 5000
```

### Par√°metros clave:
- `--timesteps`: Total de pasos (100K-200K recomendado)
- `--inner-steps`: Pasos internos por acci√≥n (3-10)
  - Menor = decisiones m√°s r√°pidas
  - Mayor = decisiones m√°s informadas
  
- `--max-steps`: M√°ximo pasos por episodio (30-100)
  - Menor = episodios cortos, aprende r√°pido
  - Mayor = episodios largos, m√°s contexto

- `--lr`: Learning rate (1e-4 a 5e-4)
  - Menor = m√°s estable pero lento
  - Mayor = m√°s r√°pido pero inestable

### Ventajas:
‚úÖ Control total de hiperpar√°metros  
‚úÖ Puedes experimentar  
‚úÖ Explorar nuevas configuraciones  

### Desventajas:
‚ö†Ô∏è Tarda m√°s (empieza de cero)  
‚ö†Ô∏è Puede no mejorar sobre el 30K  
‚ö†Ô∏è Requiere conocimiento t√©cnico  

---

## üéØ Recomendaci√≥n Estrat√©gica

### Para mejora r√°pida (2-4 horas):
```bash
# 1. Continuar RL 50K pasos m√°s
python continue_training_rl.py --timesteps 50000
```

### Para mejor calidad (4-8 horas):
```bash
# 1. Entrenar base 30 √©pocas
python train_base_more.py --epochs 30

# 2. Luego entrenar RL nuevo con base mejorado
python experiments/train_phi_text_scheduler.py --timesteps 100000
```

### Para investigaci√≥n (1-2 d√≠as):
```bash
# 1. Base extendido (50 √©pocas)
python train_base_more.py --epochs 50

# 2. RL largo (200K pasos)
python experiments/train_phi_text_scheduler.py --timesteps 200000

# 3. Continuar RL con fine-tuning
python continue_training_rl.py --timesteps 100000
```

---

## üìä Monitoreo del Entrenamiento

### Ver progreso en tiempo real:
```bash
# Para RL
python check_progress.py

# Para modelo base
# Revisar archivo training_log*.txt
```

### Analizar resultados:
```bash
# Despu√©s de entrenar RL
python analyze_rl_detailed.py

# Probar generaci√≥n
python generate_with_rl_30k.py --prompt "Test" --max-length 100
```

---

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Modificar rewards del RL:

Edita `experiments/train_phi_text_scheduler.py` l√≠neas 100-108:

```python
"reward_weights": {
    "alpha": 1.0,   # ŒîC (consciousness) - Mayor = prioriza consciousness
    "beta": 0.5,    # ŒîŒ¶ (phi) - Mayor = prioriza PHI
    "gamma": 0.1,   # Œîperplexity - Mayor = prioriza fluidez
    "delta": 0.2,   # coste - Mayor = penaliza cambios
},
```

**Ejemplos:**
- **Priorizar calidad texto**: alpha=0.5, beta=0.3, gamma=0.3, delta=0.1
- **Priorizar PHI**: alpha=0.3, beta=1.0, gamma=0.1, delta=0.2
- **Balance**: alpha=1.0, beta=0.5, gamma=0.1, delta=0.2 (actual)

### Modificar arquitectura base:

Edita `train_v5_2_gpt2_lora.py` para cambiar:
- N√∫mero de capas INFINITO
- Tama√±o de embeddings
- Configuraci√≥n de LoRA
- Tama√±o de memoria

---

## üß™ Testing Durante Entrenamiento

### Cada 10K pasos RL:
```bash
python test_rl_generation.py
```

### Cada 5 √©pocas base:
```bash
python generate_phase2_text.py \
  --checkpoint models/checkpoints/infinito_phase2_best.pt \
  --prompt "Test prompt" \
  --max-length 100
```

---

## üíæ Gesti√≥n de Checkpoints

### Espacio necesario:
- Checkpoint RL: ~50MB cada uno
- Checkpoint base: ~500MB cada uno
- Logs y m√©tricas: ~100MB por entrenamiento

### Limpieza:
```bash
# Eliminar checkpoints intermedios (dejar solo best y cada 20K)
# Manual, revisar directorio outputs/
```

---

## üö® Troubleshooting

### "CUDA out of memory"
```bash
# Reducir batch size
python train_base_more.py --batch-size 8  # En vez de 16

# O para RL, reducir inner_steps
python continue_training_rl.py --timesteps 50000  # Usa config por defecto
```

### "Entrenamiento no converge"
- Reducir learning rate: `--lr 1e-4`
- Aumentar paciencia: `--patience 10`
- Verificar que CUDA funciona: `python check_cuda.py`

### "Texto sigue con repeticiones"
- Necesitas entrenar el modelo BASE m√°s √©pocas
- Aumentar repetition penalty en generaci√≥n:
  ```bash
  python generate_with_rl_30k.py \
    --prompt "Test" \
    --max-length 100 \
    --repetition-penalty 2.0  # Aumentar de 1.2 a 2.0
  ```

---

## üìà Resultados Esperados

### Despu√©s de continuar RL 50K (+30K = 80K total):
- Reward: +7.5 a +8.0 (vs +7.25 actual)
- Mejora en decisiones: +15-20%
- Tiempo: 2-4 horas

### Despu√©s de entrenar base 30 √©pocas:
- Perplexity: 40-60 (vs ~95 actual)
- Menos repeticiones: -50%
- Tiempo: 4-8 horas

### Despu√©s de pipeline completo (base + RL):
- Texto mucho m√°s fluido y natural
- Decisiones adaptativas √≥ptimas
- PHI estable en [3-6]
- Tiempo total: 8-16 horas

---

## üìù Notas Finales

1. **Guardar progreso**: Todos los scripts guardan checkpoints autom√°ticamente
2. **Interrumpir seguro**: Ctrl+C guarda el √∫ltimo checkpoint
3. **GPU requerida**: Todos los entrenamientos requieren CUDA
4. **Memoria**: M√≠nimo 8GB VRAM recomendado
5. **Paciencia**: El entrenamiento profundo lleva tiempo, pero vale la pena

---

## üéØ Comandos Quick Start

```bash
# OPCI√ìN R√ÅPIDA (2-4h): Continuar RL
python continue_training_rl.py --timesteps 50000

# OPCI√ìN CALIDAD (4-8h): Entrenar base
python train_base_more.py --epochs 30

# OPCI√ìN COMPLETA (8-16h): Base + RL
python train_base_more.py --epochs 30 && \
python experiments/train_phi_text_scheduler.py --timesteps 100000
```

¬°Buena suerte con el entrenamiento! üöÄ
