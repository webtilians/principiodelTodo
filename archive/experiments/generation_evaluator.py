#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üìä EVALUADOR COMPARATIVO DE GENERACI√ìN DE TEXTO
==============================================

Herramienta para comparar la generaci√≥n original vs mejorada
y cuantificar las mejoras en diversidad, coherencia y calidad.
"""

import sys
import os

# Configurar encoding UTF-8 para Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

import torch
import numpy as np
import argparse
import json
from datetime import datetime
from collections import Counter
import re
from transformers import GPT2Tokenizer
from infinito_v5_2_refactored import InfinitoV52Refactored
from improved_text_generation import ImprovedTextGenerator, load_model_and_tokenizer


class GenerationEvaluator:
    """Evaluador comparativo de t√©cnicas de generaci√≥n."""
    
    def __init__(self, model, tokenizer, device):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.improved_generator = ImprovedTextGenerator(model, tokenizer, device)
    
    def generate_baseline(self, prompt, max_length=100):
        """Generaci√≥n baseline (simple greedy/sampling)."""
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)
        generated_ids = input_ids.clone()
        
        with torch.no_grad():
            for _ in range(max_length):
                # Forward pass
                outputs = self.model(generated_ids)
                
                # Obtener logits del √∫ltimo token
                if isinstance(outputs, tuple):
                    logits = outputs[0][:, -1, :]
                else:
                    logits = outputs[:, -1, :]
                
                # Sampling simple con temperatura
                temperature = 0.7
                logits = logits / temperature
                probs = torch.softmax(logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # Agregar token generado
                generated_ids = torch.cat([generated_ids, next_token], dim=-1)
                
                # Verificar fin de secuencia
                if next_token.item() == self.tokenizer.eos_token_id:
                    break
        
        # Decodificar resultado
        generated_text = self.tokenizer.decode(
            generated_ids[0][len(input_ids[0]):], 
            skip_special_tokens=True
        )
        
        return generated_text
    
    def calculate_text_metrics(self, text):
        """Calcula m√©tricas detalladas de un texto."""
        if not text or not text.strip():
            return {
                'ttr': 0.0,
                'unique_words': 0,
                'total_words': 0,
                'avg_word_length': 0.0,
                'repetition_2gram': 0.0,
                'repetition_3gram': 0.0,
                'sentence_count': 0,
                'avg_sentence_length': 0.0
            }
        
        # Preprocessing
        words = re.findall(r'\b\w+\b', text.lower())
        sentences = re.split(r'[.!?]+', text.strip())
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # M√©tricas b√°sicas
        total_words = len(words)
        unique_words = len(set(words))
        ttr = unique_words / total_words if total_words > 0 else 0
        
        # Longitud promedio de palabras
        avg_word_length = sum(len(word) for word in words) / total_words if total_words > 0 else 0
        
        # Repetici√≥n de n-gramas
        def calculate_ngram_repetition(words, n):
            if len(words) < n:
                return 0.0
            
            ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]
            ngram_counts = Counter(ngrams)
            repeated = sum(count - 1 for count in ngram_counts.values() if count > 1)
            return repeated / len(ngrams) if ngrams else 0.0
        
        rep_2gram = calculate_ngram_repetition(words, 2)
        rep_3gram = calculate_ngram_repetition(words, 3)
        
        # M√©tricas de oraciones
        sentence_count = len(sentences)
        avg_sentence_length = sum(len(s.split()) for s in sentences) / sentence_count if sentence_count > 0 else 0
        
        return {
            'ttr': ttr,
            'unique_words': unique_words,
            'total_words': total_words,
            'avg_word_length': avg_word_length,
            'repetition_2gram': rep_2gram,
            'repetition_3gram': rep_3gram,
            'sentence_count': sentence_count,
            'avg_sentence_length': avg_sentence_length
        }
    
    def evaluate_comparison(self, prompts, num_samples=3):
        """Compara generaci√≥n baseline vs mejorada para m√∫ltiples prompts."""
        results = {
            'timestamp': datetime.now().isoformat(),
            'prompts': [],
            'summary': {}
        }
        
        all_baseline_metrics = []
        all_improved_metrics = []
        
        print(f"üìä EVALUACI√ìN COMPARATIVA DE GENERACI√ìN")
        print("="*60)
        
        for i, prompt in enumerate(prompts, 1):
            print(f"\nüéØ Prompt {i}/{len(prompts)}: '{prompt}'")
            print("-" * 50)
            
            prompt_results = {
                'prompt': prompt,
                'baseline': {'texts': [], 'metrics': []},
                'improved': {'texts': [], 'metrics': []}
            }
            
            # Generar muestras baseline
            print("üîÑ Generando baseline...")
            for j in range(num_samples):
                baseline_text = self.generate_baseline(prompt)
                metrics = self.calculate_text_metrics(baseline_text)
                
                prompt_results['baseline']['texts'].append(baseline_text)
                prompt_results['baseline']['metrics'].append(metrics)
                all_baseline_metrics.append(metrics)
                
                print(f"   Baseline {j+1}: TTR={metrics['ttr']:.3f}, Palabras={metrics['total_words']}")
            
            # Generar muestras mejoradas
            print("üîÑ Generando mejoradas...")
            improved_texts = self.improved_generator.generate_multiple_samples(
                prompt, 
                num_samples=num_samples,
                temperature=0.85,
                top_p=0.9,
                repetition_penalty=1.15,
                frequency_penalty=0.05
            )
            
            for j, improved_text in enumerate(improved_texts):
                metrics = self.calculate_text_metrics(improved_text)
                
                prompt_results['improved']['texts'].append(improved_text)
                prompt_results['improved']['metrics'].append(metrics)
                all_improved_metrics.append(metrics)
                
                print(f"   Mejorada {j+1}: TTR={metrics['ttr']:.3f}, Palabras={metrics['total_words']}")
            
            results['prompts'].append(prompt_results)
        
        # Calcular m√©tricas promedio
        def average_metrics(metrics_list):
            if not metrics_list:
                return {}
            
            keys = metrics_list[0].keys()
            return {key: sum(m[key] for m in metrics_list) / len(metrics_list) for key in keys}
        
        baseline_avg = average_metrics(all_baseline_metrics)
        improved_avg = average_metrics(all_improved_metrics)
        
        # Calcular mejoras
        improvements = {}
        for key in baseline_avg:
            if baseline_avg[key] > 0:
                if key in ['repetition_2gram', 'repetition_3gram']:
                    # Para repetici√≥n, menor es mejor
                    improvement = (baseline_avg[key] - improved_avg[key]) / baseline_avg[key] * 100
                else:
                    # Para otras m√©tricas, mayor es mejor
                    improvement = (improved_avg[key] - baseline_avg[key]) / baseline_avg[key] * 100
                improvements[key] = improvement
            else:
                improvements[key] = 0.0
        
        results['summary'] = {
            'baseline_avg': baseline_avg,
            'improved_avg': improved_avg,
            'improvements': improvements,
            'total_samples': len(all_baseline_metrics)
        }
        
        # Mostrar resumen
        self._print_summary(baseline_avg, improved_avg, improvements)
        
        return results
    
    def _print_summary(self, baseline_avg, improved_avg, improvements):
        """Imprime resumen de la comparaci√≥n."""
        print(f"\nüìà RESUMEN DE MEJORAS")
        print("="*60)
        
        metrics_info = {
            'ttr': ('Diversidad (TTR)', '‚Üë Mayor es mejor'),
            'unique_words': ('Palabras √∫nicas', '‚Üë Mayor es mejor'),
            'total_words': ('Total palabras', '‚Üí Neutro'),
            'avg_word_length': ('Long. promedio palabra', '‚Üí Neutro'),
            'repetition_2gram': ('Repetici√≥n 2-gram', '‚Üì Menor es mejor'),
            'repetition_3gram': ('Repetici√≥n 3-gram', '‚Üì Menor es mejor'),
            'sentence_count': ('N√∫mero oraciones', '‚Üí Neutro'),
            'avg_sentence_length': ('Long. promedio oraci√≥n', '‚Üí Neutro')
        }
        
        for key, (name, direction) in metrics_info.items():
            baseline_val = baseline_avg[key]
            improved_val = improved_avg[key]
            improvement = improvements[key]
            
            status = "üü¢" if improvement > 5 else "üü°" if improvement > -5 else "üî¥"
            
            print(f"{status} {name:25} | "
                  f"Base: {baseline_val:6.3f} ‚Üí "
                  f"Mejorado: {improved_val:6.3f} | "
                  f"Cambio: {improvement:+6.1f}% {direction}")
        
        # M√©tricas clave
        print(f"\nüéØ M√âTRICAS CLAVE:")
        print(f"   TTR (Diversidad):     {baseline_avg['ttr']:.3f} ‚Üí {improved_avg['ttr']:.3f} ({improvements['ttr']:+.1f}%)")
        print(f"   Repetici√≥n 2-gram:    {baseline_avg['repetition_2gram']:.3f} ‚Üí {improved_avg['repetition_2gram']:.3f} ({improvements['repetition_2gram']:+.1f}%)")
        print(f"   Repetici√≥n 3-gram:    {baseline_avg['repetition_3gram']:.3f} ‚Üí {improved_avg['repetition_3gram']:.3f} ({improvements['repetition_3gram']:+.1f}%)")
        
        # Evaluaci√≥n general
        key_improvements = [
            improvements['ttr'],
            improvements['repetition_2gram'],  # Mayor es mejor (reducci√≥n de repetici√≥n)
            improvements['repetition_3gram']   # Mayor es mejor (reducci√≥n de repetici√≥n)
        ]
        
        avg_improvement = sum(key_improvements) / len(key_improvements)
        
        if avg_improvement > 20:
            evaluation = "üåü EXCELENTE"
        elif avg_improvement > 10:
            evaluation = "‚úÖ MUY BUENO"
        elif avg_improvement > 5:
            evaluation = "üëç BUENO"
        elif avg_improvement > 0:
            evaluation = "üü° MODERADO"
        else:
            evaluation = "‚ùå NECESITA MEJORAS"
        
        print(f"\nüèÜ EVALUACI√ìN GENERAL: {evaluation}")
        print(f"   Mejora promedio clave: {avg_improvement:.1f}%")


def main():
    """Funci√≥n principal."""
    parser = argparse.ArgumentParser(description='Evaluaci√≥n comparativa de generaci√≥n de texto')
    
    parser.add_argument('model_path', help='Path al modelo (.pt)')
    parser.add_argument('--prompts', nargs='+', 
                       default=[
                           'The future of artificial intelligence',
                           'In a world where technology',
                           'Science has always been',
                           'Once upon a time in'
                       ],
                       help='Lista de prompts para evaluar')
    parser.add_argument('--samples', type=int, default=3,
                       help='N√∫mero de muestras por m√©todo')
    parser.add_argument('--output', type=str, default=None,
                       help='Archivo para guardar resultados JSON')
    parser.add_argument('--device', type=str, default='auto',
                       help='Device para inferencia')
    
    args = parser.parse_args()
    
    # Cargar modelo
    model, tokenizer, device = load_model_and_tokenizer(args.model_path, args.device)
    
    # Crear evaluador
    evaluator = GenerationEvaluator(model, tokenizer, device)
    
    # Ejecutar evaluaci√≥n
    results = evaluator.evaluate_comparison(args.prompts, args.samples)
    
    # Guardar resultados si se especifica
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nüíæ Resultados guardados en: {args.output}")
    
    print(f"\n‚úÖ Evaluaci√≥n completada!")


if __name__ == '__main__':
    main()