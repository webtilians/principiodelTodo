#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üëÅÔ∏è VISUALIZACI√ìN DE ATENCI√ìN - FASE 2 IIT TRANSFORMER
======================================================

Visualiza patrones de atenci√≥n del modelo Fase 2 para entender
por qu√© colapsa en repeticiones.
"""

import sys
import os

# Configurar encoding UTF-8 para Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import GPT2Tokenizer
from train_phase2_iit_transformer import InfinitoGPT2IITPhase2


def visualize_attention_heatmap(attention_weights, tokens, layer_idx, head_idx, save_path=None):
    """
    Visualiza matriz de atenci√≥n como heatmap.
    
    Args:
        attention_weights: (seq_len, seq_len) - pesos de atenci√≥n
        tokens: Lista de tokens
        layer_idx: √çndice de la capa
        head_idx: √çndice de la cabeza de atenci√≥n
        save_path: Ruta para guardar imagen (opcional)
    """
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # Crear heatmap
    sns.heatmap(
        attention_weights,
        xticklabels=tokens,
        yticklabels=tokens,
        cmap='viridis',
        cbar=True,
        square=True,
        ax=ax,
        vmin=0,
        vmax=attention_weights.max()
    )
    
    ax.set_title(f'Attention Heatmap - Layer {layer_idx}, Head {head_idx}', fontsize=14, pad=20)
    ax.set_xlabel('Key Position', fontsize=12)
    ax.set_ylabel('Query Position', fontsize=12)
    
    plt.xticks(rotation=45, ha='right', fontsize=8)
    plt.yticks(rotation=0, fontsize=8)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  üíæ Guardado: {save_path}")
    
    plt.close()


def visualize_attention_patterns(attention_weights, tokens, layer_idx, save_path=None):
    """
    Visualiza patrones de atenci√≥n agregados de todas las cabezas.
    
    Args:
        attention_weights: (num_heads, seq_len, seq_len)
        tokens: Lista de tokens
        layer_idx: √çndice de la capa
        save_path: Ruta para guardar imagen (opcional)
    """
    # Promedio de todas las cabezas
    avg_attention = attention_weights.mean(axis=0)
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # 1. Atenci√≥n promedio
    sns.heatmap(avg_attention, ax=axes[0], cmap='viridis', cbar=True, square=True,
                xticklabels=tokens, yticklabels=tokens)
    axes[0].set_title(f'Average Attention - Layer {layer_idx}', fontsize=12)
    axes[0].set_xlabel('Key Position')
    axes[0].set_ylabel('Query Position')
    plt.setp(axes[0].get_xticklabels(), rotation=45, ha='right', fontsize=7)
    plt.setp(axes[0].get_yticklabels(), rotation=0, fontsize=7)
    
    # 2. Entrop√≠a por posici√≥n (diversidad de atenci√≥n)
    entropy = -np.sum(avg_attention * np.log(avg_attention + 1e-9), axis=1)
    axes[1].plot(entropy, marker='o', color='coral')
    axes[1].set_title('Attention Entropy per Position', fontsize=12)
    axes[1].set_xlabel('Position')
    axes[1].set_ylabel('Entropy (bits)')
    axes[1].grid(True, alpha=0.3)
    axes[1].set_xticks(range(len(tokens)))
    axes[1].set_xticklabels(tokens, rotation=45, ha='right', fontsize=7)
    
    # 3. Suma de atenci√≥n por posici√≥n (qu√© tokens reciben m√°s atenci√≥n)
    attention_sum = avg_attention.sum(axis=0)
    axes[2].bar(range(len(tokens)), attention_sum, color='steelblue', alpha=0.7)
    axes[2].set_title('Total Attention Received', fontsize=12)
    axes[2].set_xlabel('Token')
    axes[2].set_ylabel('Total Attention')
    axes[2].grid(True, alpha=0.3, axis='y')
    axes[2].set_xticks(range(len(tokens)))
    axes[2].set_xticklabels(tokens, rotation=45, ha='right', fontsize=7)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  üíæ Guardado: {save_path}")
    
    plt.close()


def visualize_phi_components(phi_metrics, tokens, save_path=None):
    """
    Visualiza componentes PHI por token.
    
    Args:
        phi_metrics: Dict con m√©tricas PHI
        tokens: Lista de tokens
        save_path: Ruta para guardar imagen (opcional)
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    components = [
        ('temporal_coherence', 'Temporal Coherence', axes[0, 0]),
        ('integration_strength', 'Integration Strength', axes[0, 1]),
        ('complexity', 'Complexity', axes[1, 0]),
        ('attention_diversity', 'Attention Diversity', axes[1, 1])
    ]
    
    for key, title, ax in components:
        if key in phi_metrics and phi_metrics[key] is not None:
            values = phi_metrics[key].cpu().numpy()
            if values.ndim == 2:  # (batch, seq_len)
                values = values[0]  # Tomar primer batch
            
            ax.plot(values, marker='o', color='teal', linewidth=2, markersize=6)
            ax.set_title(title, fontsize=12, fontweight='bold')
            ax.set_xlabel('Token Position')
            ax.set_ylabel('Score')
            ax.grid(True, alpha=0.3)
            ax.set_xticks(range(len(tokens)))
            ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=7)
            
            # L√≠nea de promedio
            mean_val = values.mean()
            ax.axhline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.3f}')
            ax.legend()
        else:
            ax.text(0.5, 0.5, 'No data', ha='center', va='center', fontsize=14, color='gray')
            ax.set_title(title, fontsize=12, fontweight='bold')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  üíæ Guardado: {save_path}")
    
    plt.close()


def analyze_attention(model, tokenizer, prompt, checkpoint, device='cuda', output_dir='results/attention_viz'):
    """
    Analiza y visualiza patrones de atenci√≥n del modelo.
    
    Args:
        model: Modelo InfinitoGPT2IITPhase2
        tokenizer: GPT2Tokenizer
        prompt: Texto de entrada
        checkpoint: Dict con m√©tricas del checkpoint
        device: 'cuda' o 'cpu'
        output_dir: Directorio para guardar visualizaciones
    """
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"AN√ÅLISIS DE ATENCI√ìN - FASE 2")
    print(f"{'='*70}")
    print(f"Prompt: '{prompt}'")
    print(f"{'='*70}\n")
    
    # Tokenizar
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    tokens = [tokenizer.decode([tok]) for tok in input_ids[0]]
    
    print(f"üìù Tokens ({len(tokens)}): {tokens}\n")
    
    model.eval()
    
    with torch.no_grad():
        # Forward pass manual para capturar atenciones
        batch_size, seq_len = input_ids.shape
        
        # 1. Embeddings
        position_ids = torch.arange(0, seq_len, dtype=torch.long, device=device).unsqueeze(0)
        hidden_states = model.gpt2_embedding(input_ids) + model.gpt2_position_embedding(position_ids)
        hidden_states = model.gpt2_drop(hidden_states)
        
        # 2. GPT-2 Layers (con atenciones)
        all_attentions = []
        for layer in model.gpt2_layers:
            outputs = layer(hidden_states, output_attentions=True, use_cache=False)
            hidden_states = outputs[0]
            if len(outputs) > 2:  # Verificar que hay atenci√≥n
                all_attentions.append(outputs[2])  # Attention weights est√°n en √≠ndice 2
        
        # 3. IIT Transformer Block
        iit_outputs = model.iit_transformer(hidden_states)
        iit_hidden = iit_outputs[0]
        iit_attentions = iit_outputs[1] if len(iit_outputs) > 1 else None
        
        # PHI metrics - simplificado (sin calcular, solo reportar checkpoint)
        phi_metrics = None
    
    print(f"üß† Informaci√≥n del checkpoint:")
    print(f"  PHI (validaci√≥n): {checkpoint['val_phi']:.4f}")
    print(f"  PPL (validaci√≥n): {checkpoint['val_ppl']:.2f}\n")
    
    # Visualizar atenciones GPT-2 (√∫ltimas 3 capas)
    print(f"üìä Generando visualizaciones GPT-2...")
    num_layers_to_viz = min(3, len(all_attentions))
    for i in range(-num_layers_to_viz, 0):
        layer_idx = len(all_attentions) + i
        attention = all_attentions[i][0].cpu().numpy()  # (num_heads, seq_len, seq_len)
        
        # Visualizaci√≥n agregada
        save_path = os.path.join(output_dir, f'gpt2_layer_{layer_idx}_patterns.png')
        visualize_attention_patterns(attention, tokens, layer_idx, save_path)
        
        # Visualizaci√≥n de cabezas individuales (primeras 4)
        for head_idx in range(min(4, attention.shape[0])):
            save_path = os.path.join(output_dir, f'gpt2_layer_{layer_idx}_head_{head_idx}.png')
            visualize_attention_heatmap(attention[head_idx], tokens, layer_idx, head_idx, save_path)
    
    # Visualizar atenciones IIT Transformer
    if iit_attentions is not None:
        print(f"üìä Generando visualizaciones IIT Transformer...")
        for layer_idx, attention in enumerate(iit_attentions):
            attention_np = attention[0].cpu().numpy()  # (num_heads, seq_len, seq_len)
            
            # Visualizaci√≥n agregada
            save_path = os.path.join(output_dir, f'iit_layer_{layer_idx}_patterns.png')
            visualize_attention_patterns(attention_np, tokens, f'IIT-{layer_idx}', save_path)
            
            # Visualizaci√≥n de cabezas individuales (primeras 4)
            for head_idx in range(min(4, attention_np.shape[0])):
                save_path = os.path.join(output_dir, f'iit_layer_{layer_idx}_head_{head_idx}.png')
                visualize_attention_heatmap(attention_np[head_idx], tokens, f'IIT-{layer_idx}', head_idx, save_path)
    
    # Visualizar componentes PHI (solo si tenemos m√©tricas)
    if phi_metrics is not None:
        print(f"üìä Generando visualizaciones PHI...")
        save_path = os.path.join(output_dir, f'phi_components.png')
        visualize_phi_components(phi_metrics, tokens, save_path)
    
    print(f"\n{'='*70}")
    print(f"‚úÖ VISUALIZACIONES COMPLETADAS")
    print(f"  üìÅ Directorio: {output_dir}")
    print(f"{'='*70}\n")
    
    return {
        'tokens': tokens,
        'phi_metrics': {k: v.cpu() if torch.is_tensor(v) else v for k, v in phi_metrics.items()} if phi_metrics else None,
        'gpt2_attentions': [att.cpu() for att in all_attentions],
        'iit_attentions': [att.cpu() for att in iit_attentions] if iit_attentions else None
    }


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Visualizaci√≥n de atenci√≥n - Fase 2')
    parser.add_argument('--prompt', type=str, default='The theory of consciousness')
    parser.add_argument('--checkpoint', type=str, default='models/checkpoints/infinito_phase2_best.pt')
    parser.add_argument('--output-dir', type=str, default='results/attention_viz')
    
    args = parser.parse_args()
    
    # Device
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nüñ•Ô∏è  Device: {device}")
    if device == 'cuda':
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
    
    # Cargar tokenizer
    print(f"\nüî§ Cargando tokenizer...")
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    
    # Cargar modelo
    print(f"\nüì¶ Cargando modelo Fase 2...")
    model = InfinitoGPT2IITPhase2(
        num_iit_layers=2,
        lambda_phi=1.0
    ).to(device)
    
    print(f"  üìÇ Cargando checkpoint: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    print(f"\nüìä M√©tricas del checkpoint:")
    print(f"  √âpoca: {checkpoint['epoch']}")
    print(f"  Val PHI: {checkpoint['val_phi']:.4f}")
    print(f"  Val PPL: {checkpoint['val_ppl']:.2f}")
    
    # Analizar atenci√≥n
    results = analyze_attention(
        model=model,
        tokenizer=tokenizer,
        prompt=args.prompt,
        checkpoint=checkpoint,
        device=device,
        output_dir=args.output_dir
    )
    
    # An√°lisis de patrones problem√°ticos
    print(f"\n{'='*70}")
    print(f"üîç AN√ÅLISIS DE PATRONES PROBLEM√ÅTICOS")
    print(f"{'='*70}\n")
    
    # Verificar si hay atenci√≥n colapsada (focalizada en pocos tokens)
    if results['gpt2_attentions'] and len(results['gpt2_attentions']) > 0:
        last_attention = results['gpt2_attentions'][-1].numpy()  # (seq_len, seq_len) o (num_heads, seq_len, seq_len)
        
        # Si tiene dimensi√≥n de heads, promediar
        if last_attention.ndim == 3:
            avg_attention = last_attention.mean(axis=0)  # (seq_len, seq_len)
        else:
            avg_attention = last_attention
        
        # Calcular entrop√≠a promedio
        entropy = -np.sum(avg_attention * np.log(avg_attention + 1e-9), axis=1)
        avg_entropy = entropy.mean()
        
        print(f"üìä Estad√≠sticas de Atenci√≥n (√∫ltima capa GPT-2):")
        print(f"  Entrop√≠a promedio: {avg_entropy:.4f} bits")
        print(f"  Entrop√≠a m√≠nima: {entropy.min():.4f} bits (posici√≥n {entropy.argmin()})")
        print(f"  Entrop√≠a m√°xima: {entropy.max():.4f} bits (posici√≥n {entropy.argmax()})")
        
        if avg_entropy < 2.0:
            print(f"\n  ‚ö†Ô∏è  ALERTA: Baja entrop√≠a detectada (<2.0 bits)")
            print(f"     La atenci√≥n est√° colapsada en pocos tokens")
        
        # Verificar tokens con mayor atenci√≥n
        attention_received = avg_attention.sum(axis=0)
        top_tokens = np.argsort(attention_received)[-3:][::-1]
        
        print(f"\nüìå Tokens con mayor atenci√≥n recibida:")
        for idx in top_tokens:
            token = results['tokens'][idx]
            score = attention_received[idx]
            print(f"  [{idx}] '{token}': {score:.3f}")


if __name__ == '__main__':
    main()
