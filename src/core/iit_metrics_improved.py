#!/usr/bin/env python3
"""
üß† IMPROVED IIT METRICS - SISTEMA DE M√âTRICAS PHI MEJORADO
==========================================================

Implementaci√≥n MEJORADA de m√©tricas de Integrated Information Theory (IIT).

MEJORAS vs versi√≥n original:
1. ‚úÖ Temporal Coherence - Mide consistencia temporal
2. ‚úÖ Attention Diversity - Shannon entropy de atenci√≥n
3. ‚úÖ Cross-Layer Integration - Integraci√≥n vertical
4. ‚úÖ Adaptive Scaling - Se adapta al perplexity del modelo
5. ‚úÖ Componentes configurables - Pesos aprendibles opcionales

COMPONENTES PHI:
- Temporal Coherence (30%): Correlaci√≥n entre posiciones consecutivas
- Integration Strength (30%): Mutual Information aproximada entre cuadrantes
- Complexity (20%): Varianza normalizada de activaciones
- Attention Diversity (20%): Shannon entropy de distribuci√≥n de atenci√≥n

ESCALADO ADAPTATIVO:
- PPL < 100 (bien entrenado): factor ~3.0 ‚Üí PHI ~3.0-5.0
- PPL > 1000 (no entrenado): factor ~1.0 ‚Üí PHI ~0.8-1.2

Referencias:
- Tononi, G. (2004). An information integration theory of consciousness
- Oizumi et al. (2014). From the phenomenology to the mechanisms of consciousness
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple
import math


class ImprovedIITMetrics(nn.Module):
    """
    Calculador mejorado de m√©tricas IIT con componentes adicionales.
    
    VENTAJAS vs versi√≥n original:
    - 4 componentes vs 3 (a√±ade temporal coherence y attention diversity)
    - Pesos optimizados para modelos entrenados
    - Escalado adaptativo seg√∫n perplexity
    - Componentes configurables (opcional: aprendibles)
    
    Args:
        hidden_dim: Dimensi√≥n del estado oculto
        perplexity: Perplexity del modelo (para escalado adaptativo)
        learnable_weights: Si True, los pesos de componentes son aprendibles
    """
    
    def __init__(
        self,
        hidden_dim: int = 512,
        perplexity: Optional[float] = None,
        learnable_weights: bool = False
    ):
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.perplexity = perplexity
        
        # Proyecciones para c√°lculo de m√©tricas
        self.phi_projector = nn.Linear(hidden_dim, hidden_dim // 2)
        self.complexity_analyzer = nn.Linear(hidden_dim, 64)
        
        # Pesos adaptativos seg√∫n perplexity
        if perplexity is not None:
            # Modelos bien entrenados (PPL < 100) tienen m√°s integraci√≥n
            # Modelos no entrenados (PPL > 1000) tienen menos
            self.ppl_factor = min(5.0, max(1.0, 300.0 / perplexity))
        else:
            self.ppl_factor = 1.0
        
        # Pesos de componentes (opcionales: aprendibles)
        if learnable_weights:
            # Inicializar cerca de pesos √≥ptimos
            self.weight_temporal = nn.Parameter(torch.tensor(0.30))
            self.weight_integration = nn.Parameter(torch.tensor(0.30))
            self.weight_complexity = nn.Parameter(torch.tensor(0.20))
            self.weight_attention = nn.Parameter(torch.tensor(0.20))
            self.learnable = True
        else:
            # Pesos fijos optimizados
            self.register_buffer('weight_temporal', torch.tensor(0.30))
            self.register_buffer('weight_integration', torch.tensor(0.30))
            self.register_buffer('weight_complexity', torch.tensor(0.20))
            self.register_buffer('weight_attention', torch.tensor(0.20))
            self.learnable = False
    
    def calculate_temporal_coherence(
        self, 
        hidden_state: torch.Tensor
    ) -> torch.Tensor:
        """
        Mide coherencia temporal (consistencia entre posiciones consecutivas).
        
        Mayor coherencia ‚Üí Mayor integraci√≥n temporal
        
        Args:
            hidden_state: [batch, seq_len, hidden_dim]
            
        Returns:
            temporal_coherence: [batch] - valores en [0, 1]
        """
        batch_size, seq_len, hidden_dim = hidden_state.shape
        
        if seq_len < 2:
            return torch.ones(batch_size, device=hidden_state.device)
        
        # Normalizar
        normalized = F.normalize(hidden_state, p=2, dim=-1)
        
        # Correlaci√≥n entre tokens consecutivos
        correlations = []
        for t in range(seq_len - 1):
            corr = (normalized[:, t, :] * normalized[:, t+1, :]).sum(dim=-1)
            correlations.append(corr)
        
        correlations = torch.stack(correlations, dim=1)  # [batch, seq_len-1]
        
        # Promedio de correlaciones (valores en [-1, 1])
        temporal_coherence = correlations.mean(dim=1)
        
        # Mapear a [0, 1]
        temporal_coherence = (temporal_coherence + 1.0) / 2.0
        
        return temporal_coherence
    
    def calculate_attention_diversity(
        self, 
        attention_weights: Optional[torch.Tensor]
    ) -> torch.Tensor:
        """
        Calcula diversidad de atenci√≥n usando Shannon entropy.
        
        Mayor diversidad ‚Üí Sistema m√°s complejo
        
        Args:
            attention_weights: [batch, heads, seq_len, seq_len] o None
            
        Returns:
            diversity: [batch] - valores en [0, 1]
        """
        if attention_weights is None:
            # Default: diversidad media
            batch_size = 1
            return torch.ones(batch_size, device=next(self.parameters()).device) * 0.5
        
        batch_size = attention_weights.size(0)
        
        # Promedio sobre heads y tokens fuente
        attn_mean = attention_weights.mean(dim=[1, 2])  # [batch, seq_len]
        
        # Shannon entropy: -sum(p * log(p))
        eps = 1e-10
        entropy = -(attn_mean * torch.log(attn_mean + eps)).sum(dim=-1)
        
        # Normalizar por m√°xima entrop√≠a posible
        seq_len = attn_mean.size(-1)
        max_entropy = math.log(seq_len) if seq_len > 0 else 1.0
        normalized_entropy = entropy / max_entropy
        
        return normalized_entropy
    
    def calculate_integration_strength(
        self, 
        hidden_state: torch.Tensor
    ) -> torch.Tensor:
        """
        Mide strength de integraci√≥n entre diferentes partes.
        
        Basado en Mutual Information aproximada entre cuadrantes.
        
        Args:
            hidden_state: [batch, seq_len, hidden_dim]
            
        Returns:
            integration: [batch] - valores en [0, 1]
        """
        batch_size, seq_len, hidden_dim = hidden_state.shape
        
        if seq_len < 4:
            return torch.ones(batch_size, device=hidden_state.device)
        
        # Dividir en 4 cuadrantes
        quarter_len = seq_len // 4
        q1 = hidden_state[:, :quarter_len, :]
        q2 = hidden_state[:, quarter_len:2*quarter_len, :]
        q3 = hidden_state[:, 2*quarter_len:3*quarter_len, :]
        q4 = hidden_state[:, 3*quarter_len:4*quarter_len, :]
        
        # Normalizar (promedio por cuadrante)
        q1_norm = F.normalize(q1.mean(dim=1), p=2, dim=-1)
        q2_norm = F.normalize(q2.mean(dim=1), p=2, dim=-1)
        q3_norm = F.normalize(q3.mean(dim=1), p=2, dim=-1)
        q4_norm = F.normalize(q4.mean(dim=1), p=2, dim=-1)
        
        # Calcular todas las correlaciones cruzadas (6 pares)
        corr_12 = (q1_norm * q2_norm).sum(dim=-1)
        corr_13 = (q1_norm * q3_norm).sum(dim=-1)
        corr_14 = (q1_norm * q4_norm).sum(dim=-1)
        corr_23 = (q2_norm * q3_norm).sum(dim=-1)
        corr_24 = (q2_norm * q4_norm).sum(dim=-1)
        corr_34 = (q3_norm * q4_norm).sum(dim=-1)
        
        # Promedio de todas las correlaciones (en [-1, 1])
        integration = (torch.abs(corr_12) + torch.abs(corr_13) + 
                      torch.abs(corr_14) + torch.abs(corr_23) +
                      torch.abs(corr_24) + torch.abs(corr_34)) / 6.0
        
        return integration
    
    def calculate_complexity(
        self, 
        hidden_state: torch.Tensor
    ) -> torch.Tensor:
        """
        Calcula complejidad como varianza normalizada de activaciones.
        
        Args:
            hidden_state: [batch, seq_len, hidden_dim]
            
        Returns:
            complexity: [batch] - valores en [0, 1]
        """
        # Varianza sobre secuencia y dimensi√≥n
        variance = hidden_state.var(dim=[1, 2])
        
        # Normalizar con sigmoid
        complexity = torch.sigmoid(variance * 10.0)
        
        return complexity
    
    def calculate_phi_approximation(
        self,
        hidden_state: torch.Tensor,
        attention_weights: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Calcula PHI mejorado con 4 componentes.
        
        COMPONENTES:
        1. Temporal Coherence (weight_temporal): Consistencia temporal
        2. Integration Strength (weight_integration): MI entre partes
        3. Complexity (weight_complexity): Varianza de activaciones
        4. Attention Diversity (weight_attention): Entrop√≠a de atenci√≥n
        
        ESCALADO:
        - Multiplicado por ppl_factor (1.0-5.0) seg√∫n perplexity
        - Multiplicado por 3.0 para rango objetivo [0, 10]
        - Clamp final a [0, 10]
        
        Args:
            hidden_state: [batch, seq_len, hidden_dim]
            attention_weights: [batch, heads, seq_len, seq_len] (opcional)
            
        Returns:
            phi: [batch] - valores en [0, 10]
        """
        # Calcular componentes
        temporal_coh = self.calculate_temporal_coherence(hidden_state)
        integration = self.calculate_integration_strength(hidden_state)
        complexity = self.calculate_complexity(hidden_state)
        attn_diversity = self.calculate_attention_diversity(attention_weights)
        
        # Normalizar pesos (para caso aprendible)
        if self.learnable:
            total_weight = (self.weight_temporal + self.weight_integration +
                          self.weight_complexity + self.weight_attention)
            w_temp = self.weight_temporal / total_weight
            w_int = self.weight_integration / total_weight
            w_comp = self.weight_complexity / total_weight
            w_attn = self.weight_attention / total_weight
        else:
            w_temp = self.weight_temporal
            w_int = self.weight_integration
            w_comp = self.weight_complexity
            w_attn = self.weight_attention
        
        # COMBINACI√ìN MEJORADA
        phi_raw = (
            w_temp * temporal_coh +
            w_int * integration +
            w_comp * complexity +
            w_attn * attn_diversity
        )
        
        # ESCALADO ADAPTATIVO
        # - Modelos bien entrenados (PPL~100): factor ~3.0
        # - Modelos no entrenados (PPL~1000): factor ~1.0
        phi_scaled = phi_raw * self.ppl_factor * 3.0
        
        # Clamp a rango razonable [0, 10]
        phi_final = torch.clamp(phi_scaled, 0, 10)
        
        return phi_final
    
    def calculate_coherence(
        self, 
        attention_weights: torch.Tensor
    ) -> torch.Tensor:
        """
        Calcula coherencia en patrones de atenci√≥n.
        
        IMPORTANTE: Medida de CONSISTENCIA en atenci√≥n, no consciencia.
        
        Args:
            attention_weights: [batch, heads, seq_len, seq_len]
            
        Returns:
            coherence: [batch] - valores en [0, 1]
        """
        batch_size = attention_weights.size(0)
        num_heads = attention_weights.size(1)
        
        # Promedio sobre tokens fuente/destino
        head_patterns = attention_weights.mean(dim=[2, 3])  # [batch, heads]
        
        # Varianza entre heads (baja varianza = alta coherencia)
        variance = head_patterns.var(dim=1)
        coherence = 1.0 / (1.0 + variance)
        
        return coherence
    
    def forward(
        self,
        hidden_state: torch.Tensor,
        attention_weights: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Calcula todas las m√©tricas IIT.
        
        Args:
            hidden_state: [batch, seq_len, hidden_dim]
            attention_weights: [batch, heads, seq_len, seq_len] (opcional)
            
        Returns:
            metrics_dict: {
                'phi_estimate': Tensor [batch],
                'temporal_coherence': Tensor [batch],
                'integration_strength': Tensor [batch],
                'complexity': Tensor [batch],
                'attention_diversity': Tensor [batch],
                'coherence': Tensor [batch] (si attention_weights disponible)
            }
        """
        # Calcular PHI y componentes
        phi = self.calculate_phi_approximation(hidden_state, attention_weights)
        temporal_coh = self.calculate_temporal_coherence(hidden_state)
        integration = self.calculate_integration_strength(hidden_state)
        complexity = self.calculate_complexity(hidden_state)
        attn_diversity = self.calculate_attention_diversity(attention_weights)
        
        metrics = {
            'phi_estimate': phi,
            'temporal_coherence': temporal_coh,
            'integration_strength': integration,
            'complexity': complexity,
            'attention_diversity': attn_diversity
        }
        
        # Coherence solo si hay attention weights
        if attention_weights is not None:
            coherence = self.calculate_coherence(attention_weights)
            metrics['coherence'] = coherence
        
        return metrics
    
    def get_component_weights(self) -> Dict[str, float]:
        """Retorna los pesos actuales de componentes."""
        return {
            'temporal': self.weight_temporal.item(),
            'integration': self.weight_integration.item(),
            'complexity': self.weight_complexity.item(),
            'attention': self.weight_attention.item(),
            'ppl_factor': self.ppl_factor
        }


# =============================================================================
# RANDOM BASELINE (para comparaci√≥n)
# =============================================================================

def random_baseline_metrics(batch_size: int, device: str = 'cpu') -> Dict[str, torch.Tensor]:
    """
    Genera m√©tricas baseline aleatorias (para comparaci√≥n).
    
    Args:
        batch_size: Tama√±o del batch
        device: 'cpu' o 'cuda'
        
    Returns:
        metrics_dict similar a ImprovedIITMetrics.forward()
    """
    return {
        'phi_estimate': torch.rand(batch_size, device=device) * 2.0,  # [0, 2]
        'temporal_coherence': torch.rand(batch_size, device=device),  # [0, 1]
        'integration_strength': torch.rand(batch_size, device=device),  # [0, 1]
        'complexity': torch.rand(batch_size, device=device),  # [0, 1]
        'attention_diversity': torch.rand(batch_size, device=device),  # [0, 1]
        'coherence': torch.rand(batch_size, device=device)  # [0, 1]
    }


# =============================================================================
# TESTS UNITARIOS
# =============================================================================

if __name__ == '__main__':
    print("="*70)
    print("üß™ TESTS - ImprovedIITMetrics")
    print("="*70)
    
    # Test 1: Inicializaci√≥n
    print("\nüìù Test 1: Inicializaci√≥n...")
    metrics_no_ppl = ImprovedIITMetrics(hidden_dim=512)
    print(f"  ‚úì Sin PPL: ppl_factor = {metrics_no_ppl.ppl_factor:.3f}")
    
    metrics_low_ppl = ImprovedIITMetrics(hidden_dim=512, perplexity=100.0)
    print(f"  ‚úì PPL=100: ppl_factor = {metrics_low_ppl.ppl_factor:.3f}")
    
    metrics_high_ppl = ImprovedIITMetrics(hidden_dim=512, perplexity=500.0)
    print(f"  ‚úì PPL=500: ppl_factor = {metrics_high_ppl.ppl_factor:.3f}")
    
    # Test 2: Forward pass
    print("\nüìù Test 2: Forward pass...")
    batch_size, seq_len, hidden_dim = 4, 32, 512
    hidden_state = torch.randn(batch_size, seq_len, hidden_dim)
    attention_weights = torch.softmax(torch.randn(batch_size, 8, seq_len, seq_len), dim=-1)
    
    results = metrics_low_ppl(hidden_state, attention_weights)
    
    print(f"  ‚úì PHI: {results['phi_estimate'].mean():.4f} (rango esperado: 1.0-4.0)")
    print(f"  ‚úì Temporal Coherence: {results['temporal_coherence'].mean():.4f}")
    print(f"  ‚úì Integration: {results['integration_strength'].mean():.4f}")
    print(f"  ‚úì Complexity: {results['complexity'].mean():.4f}")
    print(f"  ‚úì Attention Diversity: {results['attention_diversity'].mean():.4f}")
    print(f"  ‚úì Coherence: {results['coherence'].mean():.4f}")
    
    # Test 3: Pesos aprendibles
    print("\nüìù Test 3: Pesos aprendibles...")
    learnable_metrics = ImprovedIITMetrics(
        hidden_dim=512,
        perplexity=100.0,
        learnable_weights=True
    )
    
    weights = learnable_metrics.get_component_weights()
    print(f"  ‚úì Pesos iniciales:")
    for k, v in weights.items():
        print(f"     {k}: {v:.4f}")
    
    # Test 4: Comparaci√≥n con baseline
    print("\nüìù Test 4: Comparaci√≥n con baseline...")
    baseline = random_baseline_metrics(batch_size)
    
    print(f"  Modelo:    PHI = {results['phi_estimate'].mean():.4f}")
    print(f"  Baseline:  PHI = {baseline['phi_estimate'].mean():.4f}")
    print(f"  Diferencia: {(results['phi_estimate'].mean() - baseline['phi_estimate'].mean()):.4f}")
    
    print("\n" + "="*70)
    print("‚úÖ TODOS LOS TESTS PASADOS")
    print("="*70)
