"""
üß† INFINITO Knowledge Curator (El Jardinero)
============================================
Agente de mantenimiento cognitivo que estructura la memoria durante
los periodos de inactividad ("sue√±o").

Funciones:
1. MERGE: Fusiona memorias redundantes en conceptos de mayor densidad (Phi).
2. PRUNE: Elimina memorias de bajo valor (ruido) que no se usan.
3. REINFORCE: Detecta conexiones latentes entre conceptos distantes.

El algoritmo est√° inspirado en la consolidaci√≥n de memoria durante el sue√±o REM,
donde el cerebro reorganiza y fortalece conexiones importantes mientras
elimina informaci√≥n irrelevante.

Uso:
    from src.core.knowledge_curator import KnowledgeCurator
    
    curator = KnowledgeCurator(model, extractor, vectordb)
    logs = curator.sleep_cycle(max_steps=5)
"""

import torch
import numpy as np
import time
from typing import List, Dict, Tuple, Optional
from datetime import datetime


class KnowledgeCurator:
    """
    Agente aut√≥nomo de mantenimiento cognitivo.
    
    Implementa un "Algoritmo de Sue√±o" que:
    - Fusiona memorias similares en conceptos de mayor densidad Œ¶
    - Elimina memorias de bajo valor sem√°ntico (ruido)
    - Busca reducir la entrop√≠a de la memoria
    """
    
    def __init__(
        self, 
        model, 
        extractor,
        vectordb,
        device: str = None,
        similarity_threshold_merge: float = 0.82,
        phi_improvement_threshold: float = 0.05,
        prune_threshold: float = 0.3
    ):
        """
        Args:
            model: InfinitoGPT2WithObserver con .gpt2 y .tokenizer
            extractor: Layer4Extractor para embeddings y PHI
            vectordb: ConsciousVectorDB o similar con .collection
            device: 'cuda' o 'cpu'
            similarity_threshold_merge: Umbral de similitud para fusionar (0.82 = muy parecidos)
            phi_improvement_threshold: Mejora m√≠nima de PHI para aceptar fusi√≥n (5%)
            prune_threshold: PHI m√≠nimo para que un recuerdo sobreviva
        """
        self.model = model
        self.extractor = extractor
        self.db = vectordb
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = model.tokenizer
        
        # Umbrales Heur√≠sticos (Pol√≠tica Inicial del Agente)
        self.similarity_threshold_merge = similarity_threshold_merge
        self.phi_improvement_threshold = phi_improvement_threshold
        self.prune_threshold = prune_threshold
        
        # Estad√≠sticas del agente
        self.stats = {
            'total_merges': 0,
            'total_prunes': 0,
            'total_cycles': 0,
            'phi_gained': 0.0
        }
        
        print("üïµÔ∏è‚Äç‚ôÇÔ∏è Curator Agent inicializado y listo para el ciclo de sue√±o.")
        print(f"   - Umbral fusi√≥n: {similarity_threshold_merge}")
        print(f"   - Mejora PHI m√≠nima: {phi_improvement_threshold*100}%")
        print(f"   - PHI m√≠nimo supervivencia: {prune_threshold}")
        
        # Patrones triviales comunes
        self.trivial_patterns = [
            'hola', 'ok', 'vale', 'si', 'no', 'gracias', 'adios', 'bye',
            'hey', 'buenas', 'buenos dias', 'buenas noches', 'hello',
            'jaja', 'jeje', 'lol', 'xd', 'wow', 'oye', 'mira',
            'test', 'prueba', 'asdf', 'qwerty'
        ]

    def _is_trivial(self, text: str) -> bool:
        """
        Detecta si un texto es trivial (bajo valor sem√°ntico).
        
        Criterios:
        - Muy corto (< 10 caracteres)
        - Coincide con patrones triviales conocidos
        - Solo emojis o s√≠mbolos
        """
        text_clean = text.strip().lower()
        
        # Muy corto
        if len(text_clean) < 10:
            return True
        
        # Coincide con patrones triviales
        for pattern in self.trivial_patterns:
            if text_clean == pattern or text_clean.startswith(pattern + ' '):
                return True
        
        # Pocas palabras √∫nicas (repetici√≥n)
        words = text_clean.split()
        if len(words) < 3:
            return True
        
        unique_ratio = len(set(words)) / len(words)
        if unique_ratio < 0.3:  # Muy repetitivo
            return True
            
        return False

    def _calculate_similarity(self, vec_a: np.ndarray, vec_b: np.ndarray) -> float:
        """Coseno similitud entre dos vectores numpy normalizados."""
        norm_a = np.linalg.norm(vec_a)
        norm_b = np.linalg.norm(vec_b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return float(np.dot(vec_a, vec_b) / (norm_a * norm_b))

    def synthesize_concepts(self, text_a: str, text_b: str) -> str:
        """
        Pide al modelo (LLM) que fusione dos textos en una verdad superior.
        Usa un prompt de 'Reflexi√≥n' con temperatura baja para s√≠ntesis precisa.
        
        Args:
            text_a: Primer concepto
            text_b: Segundo concepto
            
        Returns:
            Texto sintetizado que combina ambos conceptos
        """
        prompt = f"""Tarea: Sintetizar y abstraer conocimiento.

Concepto A: {text_a}

Concepto B: {text_b}

Instrucci√≥n: Genera un √∫nico p√°rrafo denso que combine la informaci√≥n esencial de A y B, eliminando redundancias y elevando el nivel t√©cnico. Mant√©n solo la informaci√≥n m√°s importante.

S√≠ntesis:"""
        
        # Generaci√≥n determinista (baja temperatura) para s√≠ntesis precisa
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            output_ids = self.model.gpt2.generate(
                inputs.input_ids,
                max_new_tokens=150,
                temperature=0.3,  # Fr√≠o y anal√≠tico
                repetition_penalty=1.2,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
            
        full_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        
        # Extraer solo la s√≠ntesis
        if "S√≠ntesis:" in full_text:
            synthesis = full_text.split("S√≠ntesis:")[-1].strip()
            # Limpiar posibles artefactos
            synthesis = synthesis.split("\n\n")[0].strip()
            return synthesis
        
        return full_text.strip()

    def measure_phi(self, text: str) -> float:
        """
        Mide la densidad de informaci√≥n (Phi) de un texto.
        
        Args:
            text: Texto a medir
            
        Returns:
            Valor de PHI (float)
        """
        _, phi_metrics = self.extractor.extract_with_phi(text)
        return phi_metrics['phi']

    def get_memory_snapshot(self) -> Dict:
        """
        Obtiene un snapshot completo de la memoria actual.
        
        Returns:
            Dict con ids, documents, embeddings, metadatas
        """
        try:
            data = self.db.collection.get(
                include=['embeddings', 'documents', 'metadatas']
            )
            return data
        except Exception as e:
            print(f"‚ö†Ô∏è Error obteniendo snapshot: {e}")
            return {'ids': [], 'documents': [], 'embeddings': [], 'metadatas': []}

    def sleep_cycle(self, max_steps: int = 5, verbose: bool = True) -> List[str]:
        """
        Ejecuta un ciclo de mantenimiento cognitivo ("sue√±o").
        
        El algoritmo:
        1. Obtiene snapshot de toda la memoria
        2. Baraja aleatoriamente los recuerdos (como sue√±o REM)
        3. Para cada par cercano:
           - Si similitud > umbral_merge ‚Üí intenta fusionar
           - Si fusi√≥n mejora PHI > umbral ‚Üí consolida
           - Si PHI muy bajo ‚Üí poda (olvido activo)
        
        Args:
            max_steps: N√∫mero m√°ximo de operaciones (merge/prune)
            verbose: Si imprimir logs durante el proceso
            
        Returns:
            Lista de logs describiendo las acciones tomadas
        """
        logs = []
        self.stats['total_cycles'] += 1
        
        if verbose:
            print(f"\nüåô Iniciando ciclo de sue√±o #{self.stats['total_cycles']}...")
        
        # 1. Obtener snapshot de la memoria
        data = self.get_memory_snapshot()
        
        ids = data['ids']
        if len(ids) < 2:
            msg = "üí§ Memoria insuficiente para so√±ar (se necesitan +2 recuerdos)."
            logs.append(msg)
            return logs

        embeddings = np.array(data['embeddings'])
        documents = data['documents']
        metadatas = data['metadatas'] or [{}] * len(ids)
        
        if verbose:
            print(f"   üìö Memorias encontradas: {len(ids)}")
            print(f"   üìã Documentos en memoria:")
            for idx, doc in enumerate(documents):
                phi = metadatas[idx].get('phi', 'N/A') if metadatas[idx] else 'N/A'
                is_triv = self._is_trivial(doc)
                print(f"      [{idx}] \"{doc[:40]}{'...' if len(doc)>40 else ''}\" | Œ¶={phi} | trivial={is_triv}")
        
        step_count = 0
        processed_ids = set()
        
        # 2. Baraja aleatoria (como sue√±o REM)
        indices = list(range(len(ids)))
        np.random.shuffle(indices)
        
        if verbose:
            print(f"\n   üé≤ Orden aleatorio de procesamiento: {indices[:10]}{'...' if len(indices)>10 else ''}")

        for i in range(len(indices) - 1):
            if step_count >= max_steps:
                if verbose:
                    print(f"   ‚èπÔ∏è L√≠mite de operaciones alcanzado ({max_steps})")
                break
                
            idx_a = indices[i]
            id_a = ids[idx_a]
            doc_a = documents[idx_a]
            
            if id_a in processed_ids:
                continue
            
            if verbose:
                print(f"\n   --- Evaluando documento [{idx_a}]: \"{doc_a[:30]}...\" ---")

            # Buscar el vecino m√°s cercano en este batch aleatorio
            best_sim = -1
            best_idx_b = -1
            
            for j in range(i + 1, min(i + 10, len(indices))):  # Limitar b√∫squeda local
                idx_b = indices[j]
                id_b = ids[idx_b]
                if id_b in processed_ids:
                    continue
                
                sim = self._calculate_similarity(embeddings[idx_a], embeddings[idx_b])
                if sim > best_sim:
                    best_sim = sim
                    best_idx_b = idx_b
            
            if verbose:
                print(f"      Mejor vecino: [{best_idx_b}] con similitud={best_sim:.3f}")
            
            if best_idx_b == -1:
                if verbose:
                    print(f"      ‚ùå Sin vecino v√°lido, saltando")
                continue
                
            # --- L√ìGICA DE DECISI√ìN DEL AGENTE ---
            
            if verbose:
                print(f"      üìä Umbrales: merge>{self.similarity_threshold_merge:.2f}, prune<{self.prune_threshold:.2f}")
            
            # CASO 0: PRUNE PRIORITARIO - Eliminar trivialidades ANTES de considerar merge
            is_trivial_a = self._is_trivial(doc_a)
            if is_trivial_a:
                if verbose:
                    print(f"      üö® TRIVIALIDAD DETECTADA - Evaluando eliminaci√≥n prioritaria")
                    print(f"         - Doc: \"{doc_a}\"")
                    print(f"         - is_trivial: {is_trivial_a}")
                
                try:
                    self.db.collection.delete(ids=[id_a])
                    processed_ids.add(id_a)
                    self.stats['total_prunes'] += 1
                    
                    log_msg = (
                        f"üóëÔ∏è OLVIDO ACTIVO (Prioritario):\n"
                        f"   Recuerdo: \"{doc_a[:50]}{'...' if len(doc_a) > 50 else ''}\"\n"
                        f"   Raz√≥n: trivialidad detectada"
                    )
                    logs.append(log_msg)
                    step_count += 1
                    
                    if verbose:
                        print(f"      ‚úÖ ELIMINADO por trivialidad")
                except Exception as e:
                    if verbose:
                        print(f"      ‚ö†Ô∏è Error eliminando: {e}")
                continue  # Saltar al siguiente documento
            
            # CASO 1: MERGE (Alta redundancia detectada)
            if best_sim > self.similarity_threshold_merge:
                idx_b = best_idx_b
                id_b = ids[idx_b]
                
                doc_b = documents[idx_b]
                
                # Obtener PHI de los metadatos o calcularlo
                phi_a = metadatas[idx_a].get('phi', self.measure_phi(doc_a)) if metadatas[idx_a] else self.measure_phi(doc_a)
                phi_b = metadatas[idx_b].get('phi', self.measure_phi(doc_b)) if metadatas[idx_b] else self.measure_phi(doc_b)
                
                if verbose:
                    print(f"\n   üîç Candidatos a fusi√≥n (Sim: {best_sim:.3f}):")
                    print(f"      A: {doc_a[:50]}...")
                    print(f"      B: {doc_b[:50]}...")
                
                # Intentar sintetizar
                try:
                    synthesis = self.synthesize_concepts(doc_a, doc_b)
                    new_phi = self.measure_phi(synthesis)
                except Exception as e:
                    logs.append(f"‚ö†Ô∏è Error en s√≠ntesis: {e}")
                    continue
                
                # Criterio de Aceptaci√≥n: ¬øEs la s√≠ntesis mejor que las partes?
                max_prev_phi = max(phi_a, phi_b)
                phi_improvement = (new_phi - max_prev_phi) / max(max_prev_phi, 0.001)
                
                if phi_improvement > self.phi_improvement_threshold:
                    # ¬°√âXITO! Consolidaci√≥n de memoria
                    try:
                        # A√±adir nuevo documento consolidado
                        new_emb, new_phi_metrics = self.extractor.extract_with_phi(synthesis)
                        
                        # Crear ID √∫nico para el documento consolidado
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        new_id = f"consolidated_{timestamp}"
                        
                        # Combinar categor√≠as de los originales
                        cat_a = metadatas[idx_a].get('category', 'general') if metadatas[idx_a] else 'general'
                        cat_b = metadatas[idx_b].get('category', 'general') if metadatas[idx_b] else 'general'
                        new_category = cat_a if cat_a == cat_b else f"{cat_a}+{cat_b}"
                        
                        # A√±adir documento consolidado
                        self.db.collection.add(
                            embeddings=[new_emb.tolist()],
                            documents=[synthesis],
                            metadatas=[{
                                'phi': new_phi,
                                'category': new_category,
                                'consolidated_from': f"{id_a},{id_b}",
                                'consolidated_at': datetime.now().isoformat(),
                                'phi_gain': new_phi - max_prev_phi
                            }],
                            ids=[new_id]
                        )
                        
                        # Eliminar documentos originales
                        self.db.collection.delete(ids=[id_a, id_b])
                        
                        processed_ids.add(id_a)
                        processed_ids.add(id_b)
                        
                        # Actualizar estad√≠sticas
                        self.stats['total_merges'] += 1
                        self.stats['phi_gained'] += (new_phi - max_prev_phi)
                        
                        log_msg = (
                            f"‚ú® FUSI√ìN EXITOSA (Sim: {best_sim:.2f}):\n"
                            f"   A: {doc_a[:40]}...\n"
                            f"   B: {doc_b[:40]}...\n"
                            f"   ‚Üí S√≠ntesis: {synthesis[:60]}...\n"
                            f"   ‚Üí Nuevo Œ¶: {new_phi:.3f} (+{(new_phi-max_prev_phi):.3f})"
                        )
                        logs.append(log_msg)
                        step_count += 1
                        
                        if verbose:
                            print(f"   ‚ú® ¬°Fusi√≥n exitosa! Œ¶: {max_prev_phi:.3f} ‚Üí {new_phi:.3f}")
                            
                    except Exception as e:
                        logs.append(f"‚ö†Ô∏è Error guardando fusi√≥n: {e}")
                        
                else:
                    log_msg = f"üìâ Fusi√≥n descartada (Œ¶ {new_phi:.3f} no mejora suficiente sobre {max_prev_phi:.3f})"
                    logs.append(log_msg)
                    if verbose:
                        print(f"   üìâ Fusi√≥n descartada (mejora insuficiente)")

            # CASO 2: PRUNE (Limpieza de ruido) - solo si no hubo fusi√≥n
            elif best_sim < 0.5:  # No muy similar a nada ‚Üí posible ruido
                phi_a = metadatas[idx_a].get('phi', 0) if metadatas[idx_a] else 0
                
                # Detectar trivialidades por longitud y contenido
                is_trivial = self._is_trivial(doc_a)
                
                if verbose:
                    print(f"      üîç EVALUANDO PRUNE:")
                    print(f"         - Doc: \"{doc_a}\"")
                    print(f"         - is_trivial: {is_trivial}")
                    print(f"         - phi_a: {phi_a}")
                    print(f"         - prune_threshold: {self.prune_threshold}")
                    print(f"         - Condici√≥n trivial: {is_trivial}")
                    print(f"         - Condici√≥n phi bajo: {phi_a < self.prune_threshold and phi_a > 0}")
                
                if is_trivial or (phi_a < self.prune_threshold and phi_a > 0):
                    if verbose:
                        print(f"      ‚úÖ SER√Å ELIMINADO")
                    try:
                        self.db.collection.delete(ids=[id_a])
                        processed_ids.add(id_a)
                        
                        self.stats['total_prunes'] += 1
                        
                        reason = "trivialidad detectada" if is_trivial else f"bajo Œ¶ ({phi_a:.3f})"
                        log_msg = (
                            f"üóëÔ∏è OLVIDO ACTIVO:\n"
                            f"   Recuerdo: \"{doc_a[:50]}{'...' if len(doc_a) > 50 else ''}\"\n"
                            f"   Raz√≥n: {reason}"
                        )
                        logs.append(log_msg)
                        step_count += 1
                        
                        if verbose:
                            print(f"   üóëÔ∏è Recuerdo eliminado - Raz√≥n: {reason}")
                            
                    except Exception as e:
                        logs.append(f"‚ö†Ô∏è Error en poda: {e}")
                else:
                    if verbose:
                        print(f"      ‚ùå NO ELIMINADO - No cumple criterios de poda")
            
            # CASO 3: Ni merge ni prune - similitud intermedia
            else:
                if verbose:
                    print(f"      ‚è≠Ô∏è SALTADO - Similitud intermedia ({best_sim:.2f}), no merge ni prune")

        # Resumen final
        if not logs:
            logs.append("üí§ Sue√±o tranquilo. No se requirieron cambios estructurales.")
        
        if verbose:
            print(f"\n   üìä Ciclo completado:")
            print(f"      - Fusiones: {self.stats['total_merges']}")
            print(f"      - Podas: {self.stats['total_prunes']}")
            print(f"      - Œ¶ ganado total: {self.stats['phi_gained']:.3f}")
            
        return logs

    def get_stats(self) -> Dict:
        """Retorna estad√≠sticas del agente."""
        return self.stats.copy()
    
    def analyze_memory_health(self) -> Dict:
        """
        Analiza la salud general de la memoria.
        
        Returns:
            Dict con m√©tricas de salud:
            - total_memories: N√∫mero total de recuerdos
            - avg_phi: PHI promedio
            - entropy: Entrop√≠a estimada (dispersi√≥n de similitudes)
            - redundancy_score: Proporci√≥n de pares muy similares
            - low_phi_count: Recuerdos con PHI bajo
        """
        data = self.get_memory_snapshot()
        
        if len(data['ids']) == 0:
            return {
                'total_memories': 0,
                'avg_phi': 0,
                'entropy': 0,
                'redundancy_score': 0,
                'low_phi_count': 0
            }
        
        embeddings = np.array(data['embeddings'])
        metadatas = data['metadatas'] or [{}] * len(data['ids'])
        
        # PHI promedio
        phis = [m.get('phi', 0) for m in metadatas if m]
        avg_phi = np.mean(phis) if phis else 0
        
        # Contar PHI bajo
        low_phi_count = sum(1 for p in phis if p < self.prune_threshold)
        
        # Calcular redundancia (pares con alta similitud)
        n = len(embeddings)
        high_sim_pairs = 0
        total_pairs = 0
        similarities = []
        
        for i in range(min(n, 50)):  # Limitar para rendimiento
            for j in range(i + 1, min(n, 50)):
                sim = self._calculate_similarity(embeddings[i], embeddings[j])
                similarities.append(sim)
                total_pairs += 1
                if sim > self.similarity_threshold_merge:
                    high_sim_pairs += 1
        
        redundancy_score = high_sim_pairs / max(total_pairs, 1)
        
        # Entrop√≠a aproximada (varianza de similitudes)
        entropy = np.std(similarities) if similarities else 0
        
        return {
            'total_memories': n,
            'avg_phi': float(avg_phi),
            'entropy': float(entropy),
            'redundancy_score': float(redundancy_score),
            'low_phi_count': low_phi_count,
            'health_score': min(1.0, avg_phi * (1 - redundancy_score))
        }


# --- FUNCIONES DE UTILIDAD ---

def run_curator_demo():
    """Demo standalone del curator."""
    import os
    import sys
    
    # Setup paths
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    sys.path.insert(0, project_root)
    
    from src.core.layer4_embeddings import Layer4Extractor
    from src.core.conscious_vectordb import ConsciousVectorDB
    
    print("üß† INFINITO Knowledge Curator - Demo")
    print("=" * 50)
    
    # Crear DB de prueba
    db = ConsciousVectorDB(
        collection_name="curator_demo",
        persist_directory="data/chromadb_curator_demo",
        force_recreate=True
    )
    
    # A√±adir documentos de prueba (algunos redundantes)
    test_docs = [
        "El cielo es azul debido a la dispersi√≥n de Rayleigh.",
        "El color azul del cielo se debe a la dispersi√≥n de la luz solar.",
        "Los transformers usan mecanismos de atenci√≥n para procesar secuencias.",
        "La arquitectura transformer emplea atenci√≥n multi-cabeza para procesar texto.",
        "PHI mide la integraci√≥n de informaci√≥n en sistemas complejos.",
        "Hola mundo",  # Bajo valor sem√°ntico
    ]
    
    print("\nüìö A√±adiendo documentos de prueba...")
    for doc in test_docs:
        db.add_documents([doc], metadatas=[{'category': 'test'}])
    
    print(f"   Total: {db.collection.count()} documentos")
    
    # Crear curator
    curator = KnowledgeCurator(
        model=db.model,
        extractor=db.extractor,
        vectordb=db
    )
    
    # Analizar salud
    print("\nüìä Salud de la memoria ANTES:")
    health = curator.analyze_memory_health()
    for k, v in health.items():
        print(f"   {k}: {v:.3f}" if isinstance(v, float) else f"   {k}: {v}")
    
    # Ejecutar ciclo de sue√±o
    print("\n" + "=" * 50)
    logs = curator.sleep_cycle(max_steps=5, verbose=True)
    print("=" * 50)
    
    # Mostrar logs
    print("\nüìù Diario de sue√±o:")
    for log in logs:
        print(f"   {log}")
    
    # Analizar salud despu√©s
    print("\nüìä Salud de la memoria DESPU√âS:")
    health = curator.analyze_memory_health()
    for k, v in health.items():
        print(f"   {k}: {v:.3f}" if isinstance(v, float) else f"   {k}: {v}")
    
    print(f"\n‚úÖ Demo completada. Documentos finales: {db.collection.count()}")


if __name__ == "__main__":
    run_curator_demo()
