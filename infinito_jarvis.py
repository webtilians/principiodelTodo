#!/usr/bin/env python3
"""
üîÆ INFINITO JARVIS - Asistente con Memoria Selectiva
=====================================================

Sistema completo que combina:
1. Tu modelo Infinito (Gate Din√°mico) para filtrar qu√© recordar
2. Un LLM (OpenAI/Simulaci√≥n) para generar respuestas inteligentes
3. Memoria persistente en JSON

El flujo:
  Usuario dice algo ‚Üí Infinito analiza importancia ‚Üí 
  Si importante: GUARDA ‚Üí Construye prompt con memoria ‚Üí LLM responde

Es como tener un Jarvis que RECUERDA lo importante sobre ti.
"""

import torch
import torch.nn as nn
import json
import os
import sys
from datetime import datetime

# A√±adir src al path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from infinito_v5_2_refactored import InfinitoV52Refactored


# --- CONFIGURACI√ìN LLM ---
# Cambia USE_OPENAI a True y pon tu API Key para usar GPT real
USE_OPENAI = False
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "sk-...")

# Intentar importar OpenAI si est√° disponible
openai_client = None
if USE_OPENAI:
    try:
        from openai import OpenAI
        openai_client = OpenAI(api_key=OPENAI_API_KEY)
        print("‚úÖ OpenAI conectado")
    except ImportError:
        print("‚ö†Ô∏è OpenAI no instalado. Ejecuta: pip install openai")
        USE_OPENAI = False
    except Exception as e:
        print(f"‚ö†Ô∏è Error conectando OpenAI: {e}")
        USE_OPENAI = False


# --- MODELO INFINITO (Gate Din√°mico) ---

class InfinitoDynamicChat(InfinitoV52Refactored):
    """Modelo con gate din√°mico para detectar informaci√≥n importante."""
    
    def __init__(self, *args, **kwargs):
        kwargs['use_dynamic_gate'] = False
        super().__init__(*args, **kwargs)
        
        if hasattr(self, 'memory_gate'):
            del self.memory_gate
        
        self.gate_network = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.GELU(),
            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),
            nn.GELU(),
            nn.Linear(self.hidden_dim // 4, 1)
        )
    
    def forward(self, input_ids, return_metrics=False):
        batch_size, seq_len = input_ids.shape
        
        hidden = self.token_embedding(input_ids)
        hidden = hidden + self.position_embedding[:, :seq_len, :]
        hidden = self.embedding_dropout(hidden)
        
        for attn, ff, ln1, ln2 in zip(
            self.attention_layers, self.ff_layers, 
            self.layer_norms_1, self.layer_norms_2
        ):
            attn_out, _ = attn(hidden)
            hidden = ln1(hidden + attn_out)
            ff_out = ff(hidden)
            hidden = ln2(hidden + ff_out)
        
        sentence_context = hidden.mean(dim=1)
        gate_logit = self.gate_network(sentence_context)
        gate_open_pct = torch.sigmoid(gate_logit)
        
        logits = self.output_projection(hidden)
        
        if return_metrics:
            return logits, {'gate_value': gate_open_pct.mean().item()}
        return logits, None


def text_to_ids(text, seq_len=32):
    """Convierte texto a IDs ASCII."""
    ids = [ord(c) % 256 for c in text]
    if len(ids) < seq_len:
        ids = ids + [0] * (seq_len - len(ids))
    else:
        ids = ids[:seq_len]
    return torch.tensor([ids])


# --- SISTEMA JARVIS ---

class JarvisSystem:
    """Asistente inteligente con memoria selectiva."""
    
    def __init__(self, keeper_model_path, db_file="memoria_jarvis.json"):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.db_file = db_file
        self.conversation_history = []
        
        print(f"\n{'='*60}")
        print(f"üîÆ INFINITO JARVIS")
        print(f"{'='*60}")
        print(f"Device: {self.device}")
        print(f"LLM: {'OpenAI GPT' if USE_OPENAI else 'Simulaci√≥n'}")
        
        # 1. Cargar modelo Infinito (el "portero" de la memoria)
        self._load_keeper(keeper_model_path)
        
        # 2. Cargar memorias existentes
        self._load_memories()
        
        print(f"{'='*60}\n")
    
    def _load_keeper(self, model_path):
        """Carga el modelo Infinito que decide qu√© guardar."""
        print(f"\nüß† Cargando Infinito Gate...")
        
        self.keeper = InfinitoDynamicChat(
            vocab_size=256,
            hidden_dim=64,
            num_layers=2,
            num_heads=4,
            use_improved_memory=True,
            use_improved_iit=True,
        ).to(self.device)
        
        try:
            checkpoint = torch.load(model_path, weights_only=False, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                self.keeper.load_state_dict(checkpoint['model_state_dict'], strict=False)
            else:
                self.keeper.load_state_dict(checkpoint, strict=False)
            self.keeper.eval()
            print("   ‚úÖ Modelo cargado")
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            sys.exit(1)
    
    def _load_memories(self):
        """Carga las memorias del archivo JSON."""
        if os.path.exists(self.db_file):
            with open(self.db_file, 'r', encoding='utf-8') as f:
                self.memories = json.load(f)
            print(f"üìÇ {len(self.memories)} recuerdos cargados")
        else:
            self.memories = []
            print("‚ú® Nueva memoria iniciada")
    
    def _analyze_importance(self, text):
        """Usa el modelo Infinito para medir importancia."""
        inp = text_to_ids(text).to(self.device)
        with torch.no_grad():
            _, metrics = self.keeper(inp, return_metrics=True)
        return metrics['gate_value'] * 100
    
    def _save_memory(self, text, score, category="general"):
        """Guarda un recuerdo importante."""
        entry = {
            "id": len(self.memories) + 1,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "content": text,
            "score": round(score, 1),
            "category": category
        }
        self.memories.append(entry)
        
        with open(self.db_file, 'w', encoding='utf-8') as f:
            json.dump(self.memories, f, indent=2, ensure_ascii=False)
    
    def _categorize(self, text):
        """Categoriza el tipo de informaci√≥n."""
        text_lower = text.lower()
        
        # Si es una pregunta, no es informaci√≥n para guardar
        if text_lower.endswith('?') or text_lower.startswith(('qu√©', 'que', 'c√≥mo', 'como', 'cu√°l', 'cual')):
            return "pregunta"  # Las preguntas no deber√≠an guardarse
        
        if any(x in text_lower for x in ['me llamo', 'mi nombre es', 'soy ', 'll√°mame']):
            return "identidad"
        elif any(x in text_lower for x in ['contrase√±a', 'clave', 'password', 'pin', 'secreto']):
            return "credencial"
        elif any(x in text_lower for x in ['tel√©fono', 'email', 'correo', 'direcci√≥n', 'vivo en']):
            return "contacto"
        elif any(x in text_lower for x in ['recuerda', 'no olvides', 'importante', 'ma√±ana', 'cita']):
            return "recordatorio"
        elif any(x in text_lower for x in ['me gusta', 'prefiero', 'favorito', 'odio']):
            return "preferencia"
        else:
            return "general"
    
    def _construct_system_prompt(self):
        """Construye el prompt del sistema con la memoria."""
        
        # Agrupar memorias por categor√≠a
        categories = {}
        for mem in self.memories[-10:]:  # √öltimos 10 recuerdos
            cat = mem.get('category', 'general')
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(mem['content'])
        
        # Construir bloque de memoria
        memory_block = ""
        if self.memories:
            memory_block = "\nüìö INFORMACI√ìN QUE CONOCES SOBRE EL USUARIO:\n"
            
            for cat, items in categories.items():
                emoji = {
                    'identidad': 'üë§',
                    'credencial': 'üîê',
                    'contacto': 'üìû',
                    'recordatorio': 'üìå',
                    'preferencia': '‚ù§Ô∏è',
                    'general': 'üìù'
                }.get(cat, 'üìù')
                
                memory_block += f"\n{emoji} {cat.upper()}:\n"
                for item in items:
                    memory_block += f"   ‚Ä¢ {item}\n"
        
        system_prompt = f"""Eres Infinito, un asistente personal inteligente con memoria a largo plazo.

CARACTER√çSTICAS:
- Eres amable, conciso y √∫til
- RECUERDAS informaci√≥n importante sobre el usuario
- Usas esa informaci√≥n para personalizar tus respuestas
- Si el usuario te dio su nombre, √öSALO
- Si te comparti√≥ datos, refi√©rete a ellos cuando sea relevante
{memory_block}

INSTRUCCIONES:
1. Responde de forma natural y conversacional
2. Si tienes informaci√≥n relevante en tu memoria, √öSALA
3. No inventes informaci√≥n que no tienes
4. S√© breve pero completo
"""
        return system_prompt
    
    def _get_llm_response(self, user_text, system_prompt):
        """Obtiene respuesta del LLM."""
        
        if USE_OPENAI and openai_client:
            try:
                # Construir mensajes con historial
                messages = [{"role": "system", "content": system_prompt}]
                
                # A√±adir √∫ltimos turnos de conversaci√≥n
                for turn in self.conversation_history[-4:]:
                    messages.append({"role": "user", "content": turn['user']})
                    messages.append({"role": "assistant", "content": turn['assistant']})
                
                messages.append({"role": "user", "content": user_text})
                
                response = openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=messages,
                    max_tokens=500,
                    temperature=0.7
                )
                return response.choices[0].message.content
                
            except Exception as e:
                return f"Error con OpenAI: {e}"
        else:
            # Modo simulaci√≥n - mostrar lo que ver√≠a el LLM
            return self._simulate_response(user_text, system_prompt)
    
    def _simulate_response(self, user_text, system_prompt):
        """Simula una respuesta inteligente basada en la memoria."""
        
        # Buscar informaci√≥n del usuario en memoria
        user_name = None
        user_data = {'credenciales': [], 'contactos': [], 'recordatorios': [], 'preferencias': []}
        
        for mem in self.memories:
            content = mem['content']
            content_lower = content.lower()
            category = mem.get('category', 'general')
            
            # Extraer nombre
            if category == 'identidad' or 'me llamo' in content_lower or 'mi nombre' in content_lower:
                for pattern in ['me llamo ', 'mi nombre es ', 'soy ']:
                    if pattern in content_lower:
                        idx = content_lower.find(pattern) + len(pattern)
                        name_part = content[idx:].split()[0] if content[idx:].split() else None
                        if name_part:
                            user_name = name_part.strip('.,!?¬ø¬°')
                            break
            
            # Categorizar datos
            if category == 'credencial':
                user_data['credenciales'].append(content)
            elif category == 'contacto':
                user_data['contactos'].append(content)
            elif category == 'recordatorio':
                user_data['recordatorios'].append(content)
            elif category == 'preferencia':
                user_data['preferencias'].append(content)
        
        # Generar respuesta contextual
        text_lower = user_text.lower()
        
        # Saludos
        if any(x in text_lower for x in ['hola', 'buenos', 'buenas', 'hey']):
            if user_name:
                return f"¬°Hola {user_name}! üòä ¬øEn qu√© puedo ayudarte hoy?"
            return "¬°Hola! Soy Infinito, tu asistente con memoria. ¬øC√≥mo te llamas?"
        
        # Preguntas sobre identidad
        elif any(x in text_lower for x in ['c√≥mo me llamo', 'como me llamo', 'mi nombre', 'qui√©n soy', 'quien soy']):
            if user_name:
                return f"Te llamas {user_name}. Lo recuerdo perfectamente desde que me lo dijiste üòä"
            return "Todav√≠a no me has dicho tu nombre. ¬øC√≥mo te llamas?"
        
        # Preguntas sobre memoria
        elif any(x in text_lower for x in ['qu√© sabes', 'que sabes', 'qu√© recuerdas', 'que recuerdas']):
            if self.memories:
                parts = [f"Tengo {len(self.memories)} recuerdos sobre ti"]
                if user_name:
                    parts.append(f"S√© que te llamas {user_name}")
                if user_data['credenciales']:
                    parts.append(f"Tengo {len(user_data['credenciales'])} credenciales guardadas")
                if user_data['recordatorios']:
                    parts.append(f"Tienes {len(user_data['recordatorios'])} recordatorios")
                return ". ".join(parts) + ". Escribe 'ver memoria' para ver todo."
            return "A√∫n no tengo recuerdos tuyos. ¬°Cu√©ntame algo importante sobre ti!"
        
        # Agradecimientos
        elif any(x in text_lower for x in ['gracias', 'thanks', 'thx']):
            if user_name:
                return f"¬°De nada, {user_name}! Siempre a tu servicio üôå"
            return "¬°Con gusto! Para eso estoy üòä"
        
        # Despedidas
        elif any(x in text_lower for x in ['adi√≥s', 'adios', 'chao', 'bye', 'hasta luego']):
            if user_name:
                return f"¬°Hasta pronto, {user_name}! Recordar√© todo lo que me dijiste üëã"
            return "¬°Hasta luego! Fue un gusto chatear contigo üëã"
        
        # Confirmaci√≥n de que guard√≥ algo
        elif any(x in text_lower for x in ['me llamo', 'mi nombre es', 'soy ']):
            # Extraer el nombre que acaba de decir
            for pattern in ['me llamo ', 'mi nombre es ', 'soy ']:
                if pattern in text_lower:
                    idx = text_lower.find(pattern) + len(pattern)
                    new_name = user_text[idx:].split()[0] if user_text[idx:].split() else "amigo"
                    new_name = new_name.strip('.,!?¬ø¬°')
                    return f"¬°Encantado de conocerte, {new_name}! üòä Ya guard√© tu nombre en mi memoria."
            return "¬°Encantado! Ya guard√© tu nombre."
        
        elif any(x in text_lower for x in ['contrase√±a', 'clave', 'password', 'secreto']):
            return "üîê Guardado de forma segura en mi memoria. No lo olvidar√©."
        
        elif any(x in text_lower for x in ['recuerda', 'no olvides']):
            return "üìå ¬°Anotado! Te lo recordar√© cuando sea necesario."
        
        elif any(x in text_lower for x in ['tel√©fono', 'email', 'correo']):
            return "üìû Informaci√≥n de contacto guardada. La tendr√© presente."
        
        else:
            # Respuesta gen√©rica pero personalizada
            if user_name:
                return f"Entendido, {user_name}. ¬øHay algo m√°s en lo que pueda ayudarte?"
            return "Entendido. ¬øHay algo m√°s que quieras contarme o preguntarme?"
    
    def chat(self, user_text):
        """Procesa un mensaje del usuario."""
        
        # PASO 1: Analizar importancia con Infinito
        importance = self._analyze_importance(user_text)
        is_important = importance > 50.0
        
        # Feedback visual del gate
        bar_len = int(importance / 5)
        bar = "‚ñà" * bar_len + "‚ñë" * (20 - bar_len)
        
        print(f"\n   ‚îå{'‚îÄ'*44}‚îê")
        print(f"   ‚îÇ üîç Gate: [{bar}] {importance:>6.1f}% ", end="")
        
        if is_important:
            category = self._categorize(user_text)
            # No guardar preguntas (aunque tengan alta importancia)
            if category != "pregunta":
                self._save_memory(user_text, importance, category)
                print(f"üü¢ ‚îÇ")
                print(f"   ‚îÇ üíæ Guardado como: {category:<24} ‚îÇ")
            else:
                print(f"üü° ‚îÇ")
                print(f"   ‚îÇ ‚ùì Pregunta detectada (no se guarda)      ‚îÇ")
        else:
            print(f"üî¥ ‚îÇ")
        print(f"   ‚îî{'‚îÄ'*44}‚îò")
        
        # PASO 2: Construir prompt con memoria
        system_prompt = self._construct_system_prompt()
        
        # PASO 3: Obtener respuesta del LLM
        response = self._get_llm_response(user_text, system_prompt)
        
        # Guardar en historial de conversaci√≥n
        self.conversation_history.append({
            'user': user_text,
            'assistant': response,
            'importance': importance
        })
        
        return response
    
    def show_memory(self):
        """Muestra la memoria completa."""
        print(f"\n{'='*60}")
        print(f"üß† MEMORIA DE INFINITO ({len(self.memories)} recuerdos)")
        print(f"{'='*60}")
        
        if not self.memories:
            print("   (La memoria est√° vac√≠a)")
        else:
            for mem in self.memories:
                emoji = {
                    'identidad': 'üë§',
                    'credencial': 'üîê',
                    'contacto': 'üìû',
                    'recordatorio': 'üìå',
                    'preferencia': '‚ù§Ô∏è',
                    'general': 'üìù'
                }.get(mem.get('category', 'general'), 'üìù')
                
                print(f"\n   {emoji} #{mem['id']} [{mem['timestamp']}]")
                print(f"      \"{mem['content']}\"")
                print(f"      Importancia: {mem['score']}% | Tipo: {mem.get('category', 'general')}")
        
        print(f"\n{'='*60}")
    
    def show_prompt(self):
        """Muestra el prompt actual que se enviar√≠a al LLM."""
        prompt = self._construct_system_prompt()
        print(f"\n{'='*60}")
        print("ü§ñ SYSTEM PROMPT ACTUAL (lo que ve el LLM):")
        print(f"{'='*60}")
        print(prompt)
        print(f"{'='*60}")


def print_help():
    """Muestra la ayuda."""
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  üîÆ COMANDOS DE INFINITO JARVIS                              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  ver memoria  - Muestra todos los recuerdos                  ‚ïë
‚ïë  ver prompt   - Muestra el prompt que ve el LLM              ‚ïë
‚ïë  borrar       - Borra toda la memoria                        ‚ïë
‚ïë  ayuda        - Muestra esta ayuda                           ‚ïë
‚ïë  salir        - Termina el programa                          ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  üí° PRUEBA DECIR:                                            ‚ïë
‚ïë  ‚Ä¢ "Hola, me llamo [Tu Nombre]"                              ‚ïë
‚ïë  ‚Ä¢ "La contrase√±a del wifi es 1234"                          ‚ïë
‚ïë  ‚Ä¢ "Mi email es ejemplo@mail.com"                            ‚ïë
‚ïë  ‚Ä¢ "Recuerda que ma√±ana tengo reuni√≥n"                       ‚ïë
‚ïë  ‚Ä¢ "¬øC√≥mo me llamo?" (despu√©s de decir tu nombre)            ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")


def main():
    """Funci√≥n principal."""
    
    # Ruta al modelo
    MODEL_PATH = os.path.join(os.path.dirname(__file__), "models", "dynamic_chat_detector_v2.pt")
    
    if not os.path.exists(MODEL_PATH):
        print(f"‚ùå No se encontr√≥ el modelo en: {MODEL_PATH}")
        print("   Ejecuta primero: python test_dynamic_chat_v2.py")
        sys.exit(1)
    
    # Inicializar Jarvis
    jarvis = JarvisSystem(MODEL_PATH)
    
    # Instrucciones
    print("üí¨ Chatea conmigo. Recordar√© lo importante.")
    print("   Escribe 'ayuda' para ver comandos.")
    print("‚îÄ" * 60)
    
    while True:
        try:
            user_input = input("\nüë§ T√∫ > ").strip()
            
            if not user_input:
                continue
            
            cmd = user_input.lower()
            
            # Comandos especiales
            if cmd in ['salir', 'exit', 'quit']:
                break
            elif cmd == 'ver memoria':
                jarvis.show_memory()
                continue
            elif cmd == 'ver prompt':
                jarvis.show_prompt()
                continue
            elif cmd in ['ayuda', 'help']:
                print_help()
                continue
            elif cmd == 'borrar':
                confirm = input("   ¬øBorrar toda la memoria? (s/n): ")
                if confirm.lower() == 's':
                    jarvis.memories = []
                    with open(jarvis.db_file, 'w') as f:
                        json.dump([], f)
                    print("   üóëÔ∏è Memoria borrada")
                continue
            
            # Chat normal
            response = jarvis.chat(user_input)
            print(f"\nü§ñ Infinito > {response}")
            
        except KeyboardInterrupt:
            print("\n")
            break
        except EOFError:
            break
    
    # Despedida
    print(f"\n{'='*60}")
    print(f"üëã ¬°Hasta luego!")
    print(f"   Recuerdos guardados: {len(jarvis.memories)}")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
