#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üî¨ AN√ÅLISIS DEL ENTRENAMIENTO RECIENTE: INFINITO V5.2
=====================================================

An√°lisis profundo del modelo infinito_v5.2_real_best.pt reci√©n entrenado:
- Comparaci√≥n de m√©tricas con modelos anteriores
- Evaluaci√≥n de las caracter√≠sticas IIT implementadas
- An√°lisis de la evoluci√≥n del rendimiento
- Recomendaciones para optimizaciones futuras
"""

import sys
import os

# Configurar encoding UTF-8 para Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import numpy as np
import glob
from pathlib import Path

def load_model_comparison_data():
    """Carga los datos de comparaci√≥n de modelos m√°s recientes."""
    try:
        comparison_file = "model_comparison_results_updated/models_comparison.csv"
        if os.path.exists(comparison_file):
            return pd.read_csv(comparison_file)
        else:
            print(f"‚ö†Ô∏è  Archivo de comparaci√≥n no encontrado: {comparison_file}")
            return None
    except Exception as e:
        print(f"‚ùå Error cargando comparaci√≥n: {e}")
        return None

def load_evaluation_data():
    """Carga los datos de evaluaci√≥n m√°s recientes."""
    try:
        eval_dir = "evaluation_results_latest"
        if not os.path.exists(eval_dir):
            print(f"‚ö†Ô∏è  Directorio de evaluaci√≥n no encontrado: {eval_dir}")
            return None
        
        # Buscar el archivo de evaluaci√≥n m√°s reciente
        eval_files = glob.glob(f"{eval_dir}/evaluation_*.json")
        if not eval_files:
            print(f"‚ö†Ô∏è  No se encontraron archivos de evaluaci√≥n en {eval_dir}")
            return None
        
        latest_file = max(eval_files, key=os.path.getmtime)
        print(f"üìä Cargando evaluaci√≥n: {latest_file}")
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ùå Error cargando evaluaci√≥n: {e}")
        return None

def analyze_model_performance():
    """Analiza el rendimiento del modelo reci√©n entrenado."""
    print(f"\n{'='*70}")
    print(f"üî¨ AN√ÅLISIS DEL MODELO RECI√âN ENTRENADO")
    print(f"{'='*70}")
    
    # 1. Cargar datos de comparaci√≥n
    comparison_df = load_model_comparison_data()
    if comparison_df is not None:
        # Filtrar el modelo reci√©n entrenado
        recent_model = comparison_df[comparison_df['Modelo'].str.contains('infinito_v5.2_real_best.pt', na=False)]
        
        if not recent_model.empty:
            print(f"\nüìà M√âTRICAS DEL MODELO RECIENTE:")
            print(f"{'='*50}")
            model_data = recent_model.iloc[0]
            
            print(f"  üè∑Ô∏è  Modelo: {model_data['Modelo']}")
            print(f"  üìä PPL Final: {model_data['PPL Final']:.2f}")
            print(f"  üî¢ Par√°metros: {model_data['Par√°metros']:,}")
            print(f"  üß† Hidden Dim: {model_data['Hidden Dim']}")
            print(f"  ‚öôÔ∏è  IIT Memory: {model_data['IIT Memory']}")
            print(f"  üéØ √âpoca: {model_data['√âpoca']}")
            
            # Calcular ranking
            sorted_by_ppl = comparison_df.sort_values('PPL Final')
            ranking = sorted_by_ppl.reset_index(drop=True).index[
                sorted_by_ppl['Modelo'] == 'infinito_v5.2_real_best.pt'
            ].tolist()
            
            if ranking:
                print(f"  üèÜ Ranking por PPL: #{ranking[0] + 1} de {len(comparison_df)} modelos")
        else:
            print("‚ö†Ô∏è  Modelo reciente no encontrado en comparaci√≥n")
    
    # 2. Cargar datos de evaluaci√≥n de generaci√≥n
    eval_data = load_evaluation_data()
    if eval_data is not None:
        print(f"\nüé® AN√ÅLISIS DE GENERACI√ìN DE TEXTO:")
        print(f"{'='*50}")
        
        model_info = eval_data.get('model_info', {})
        global_metrics = eval_data.get('global_metrics', {})
        
        print(f"  üîß Configuraci√≥n:")
        config = model_info.get('model_config', {})
        print(f"    - Vocab Size: {config.get('vocab_size', 'N/A'):,}")
        print(f"    - Hidden Dim: {config.get('hidden_dim', 'N/A')}")
        print(f"    - IIT Memory: {config.get('use_improved_memory', 'N/A')}")
        print(f"    - Par√°metros: {model_info.get('total_parameters', 'N/A'):,}")
        
        print(f"\n  üìä M√©tricas de Generaci√≥n:")
        if 'diversity' in global_metrics:
            diversity = global_metrics['diversity']
            print(f"    - Diversidad TTR: {diversity.get('type_token_ratio', 0):.3f}")
            print(f"    - Tokens √∫nicos: {diversity.get('unique_tokens', 0)}")
            print(f"    - Diversidad intra-texto: {diversity.get('intra_text_diversity', 0):.3f}")
            print(f"    - Diversidad inter-texto: {diversity.get('inter_text_diversity', 0):.3f}")
        
        if 'repetition' in global_metrics:
            repetition = global_metrics['repetition']
            print(f"    - Repetici√≥n 2-gram: {repetition.get('avg_repetition_2gram', 0):.3f}")
            print(f"    - Repetici√≥n 3-gram: {repetition.get('avg_repetition_3gram', 0):.3f}")
        
        if 'coherence' in global_metrics:
            coherence = global_metrics['coherence']
            print(f"    - Perplexity promedio: {coherence.get('avg_perplexity', 0):,.2f}")
            print(f"    - Puntuaci√≥n consistencia: {coherence.get('consistency_score', 0):.3f}")
            print(f"    - Puntuaci√≥n fluidez: {coherence.get('fluency_score', 0):.3f}")

def analyze_training_evolution():
    """Analiza la evoluci√≥n del entrenamiento comparando con modelos anteriores."""
    print(f"\n{'='*70}")
    print(f"üìà EVOLUCI√ìN DEL ENTRENAMIENTO")
    print(f"{'='*70}")
    
    comparison_df = load_model_comparison_data()
    if comparison_df is None:
        return
    
    # Filtrar modelos de la serie v5.2
    v52_models = comparison_df[comparison_df['Modelo'].str.contains('v5.2', na=False)]
    
    if v52_models.empty:
        print("‚ö†Ô∏è  No se encontraron modelos v5.2 para comparar")
        return
    
    print(f"\nüîÑ COMPARACI√ìN MODELOS V5.2:")
    print(f"{'='*50}")
    
    # Ordenar por PPL
    v52_sorted = v52_models.sort_values('PPL Final')
    
    for idx, (_, row) in enumerate(v52_sorted.iterrows()):
        modelo = row['Modelo'].replace('infinito_', '').replace('.pt', '')
        print(f"  {idx+1}. {modelo}")
        print(f"     PPL: {row['PPL Final']:.2f} | Params: {row['Par√°metros']:,} | √âpoca: {row['√âpoca']}")
    
    # An√°lisis de mejoras
    print(f"\nüí° OBSERVACIONES:")
    print(f"{'='*30}")
    
    best_model = v52_sorted.iloc[0]
    if 'real_best' in best_model['Modelo']:
        print(f"  ‚úÖ El modelo reci√©n entrenado ES el mejor de la serie V5.2")
        print(f"     - Mejor PPL: {best_model['PPL Final']:.2f}")
        print(f"     - Configuraci√≥n: {best_model['Hidden Dim']}d, {best_model['√âpoca']} √©pocas")
    else:
        print(f"  ‚ö†Ô∏è  El modelo reci√©n entrenado NO es el mejor de la serie")
        print(f"     - Mejor modelo: {best_model['Modelo']}")
        print(f"     - Su PPL: {best_model['PPL Final']:.2f}")
    
    # An√°lisis de eficiencia
    if 'Par√°metros' in v52_sorted.columns and 'PPL Final' in v52_sorted.columns:
        v52_sorted['Eficiencia'] = 1 / (v52_sorted['PPL Final'] * v52_sorted['Par√°metros'] / 1e6)
        best_efficiency = v52_sorted.loc[v52_sorted['Eficiencia'].idxmax()]
        
        print(f"\n‚ö° MODELO M√ÅS EFICIENTE V5.2:")
        print(f"  üèÜ {best_efficiency['Modelo'].replace('infinito_', '').replace('.pt', '')}")
        print(f"  üìä Eficiencia: {best_efficiency['Eficiencia']:.6f}")
        print(f"  üìà PPL: {best_efficiency['PPL Final']:.2f}")
        print(f"  üî¢ Par√°metros: {best_efficiency['Par√°metros']:,}")

def generate_recommendations():
    """Genera recomendaciones para optimizaciones futuras."""
    print(f"\n{'='*70}")
    print(f"üí° RECOMENDACIONES PARA OPTIMIZACI√ìN")
    print(f"{'='*70}")
    
    eval_data = load_evaluation_data()
    comparison_df = load_model_comparison_data()
    
    recommendations = []
    
    # An√°lisis basado en m√©tricas de generaci√≥n
    if eval_data:
        global_metrics = eval_data.get('global_metrics', {})
        
        # Revisar diversidad
        if 'diversity' in global_metrics:
            ttr = global_metrics['diversity'].get('type_token_ratio', 0)
            if ttr < 0.3:
                recommendations.append({
                    'category': 'üé® Diversidad',
                    'issue': f'TTR bajo ({ttr:.3f})',
                    'suggestion': 'Aumentar temperatura de sampling o usar nucleus sampling'
                })
        
        # Revisar repetici√≥n
        if 'repetition' in global_metrics:
            rep_2gram = global_metrics['repetition'].get('avg_repetition_2gram', 0)
            if rep_2gram > 0.08:
                recommendations.append({
                    'category': 'üîÑ Repetici√≥n',
                    'issue': f'Alta repetici√≥n 2-gram ({rep_2gram:.3f})',
                    'suggestion': 'Implementar penalty para repeticiones o ajustar dropout'
                })
        
        # Revisar coherencia
        if 'coherence' in global_metrics:
            perplexity = global_metrics['coherence'].get('avg_perplexity', 0)
            if perplexity > 1000:
                recommendations.append({
                    'category': 'üß† Coherencia',
                    'issue': f'Perplexity muy alta ({perplexity:,.0f})',
                    'suggestion': 'Entrenar m√°s √©pocas o reducir learning rate'
                })
    
    # An√°lisis basado en comparaci√≥n de modelos
    if comparison_df is not None:
        recent_model = comparison_df[comparison_df['Modelo'].str.contains('real_best', na=False)]
        if not recent_model.empty:
            model_data = recent_model.iloc[0]
            
            # Comparar con mejor modelo general
            best_overall = comparison_df.loc[comparison_df['PPL Final'].idxmin()]
            if model_data['PPL Final'] > best_overall['PPL Final'] * 1.1:
                recommendations.append({
                    'category': 'üìä Rendimiento',
                    'issue': f'PPL sub√≥ptima vs mejor modelo ({model_data["PPL Final"]:.2f} vs {best_overall["PPL Final"]:.2f})',
                    'suggestion': f'Probar configuraci√≥n del modelo {best_overall["Modelo"]}'
                })
    
    # Recomendaciones generales para IIT
    recommendations.extend([
        {
            'category': 'üß† IIT Features',
            'issue': 'Optimizaci√≥n de caracter√≠sticas IIT',
            'suggestion': 'Ajustar lambda_phi para mejor balance entre LM y IIT loss'
        },
        {
            'category': '‚öôÔ∏è  Arquitectura',
            'issue': 'Exploraci√≥n de configuraciones',
            'suggestion': 'Probar arquitecturas optimizadas (ultra_efficient, balanced_performance)'
        },
        {
            'category': 'üìö Datos',
            'issue': 'Expansi√≥n del dataset',
            'suggestion': 'Considerar WikiText-103 o datasets m√°s grandes para mejor generalizaci√≥n'
        }
    ])
    
    # Mostrar recomendaciones
    for i, rec in enumerate(recommendations, 1):
        print(f"\n{i}. {rec['category']}")
        print(f"   ‚ö†Ô∏è  Problema: {rec['issue']}")
        print(f"   üí° Sugerencia: {rec['suggestion']}")

def create_summary_report():
    """Crea un reporte resumen del an√°lisis."""
    print(f"\n{'='*70}")
    print(f"üìã RESUMEN EJECUTIVO")
    print(f"{'='*70}")
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"üìÖ Generado: {timestamp}")
    
    # Estado del modelo
    comparison_df = load_model_comparison_data()
    eval_data = load_evaluation_data()
    
    if comparison_df is not None:
        total_models = len(comparison_df)
        recent_model = comparison_df[comparison_df['Modelo'].str.contains('real_best', na=False)]
        
        if not recent_model.empty:
            model_ppl = recent_model.iloc[0]['PPL Final']
            model_params = recent_model.iloc[0]['Par√°metros']
            
            print(f"\nüéØ ESTADO ACTUAL:")
            print(f"  ‚Ä¢ Total de modelos analizados: {total_models}")
            print(f"  ‚Ä¢ PPL del modelo reciente: {model_ppl:.2f}")
            print(f"  ‚Ä¢ Par√°metros del modelo reciente: {model_params:,}")
    
    if eval_data:
        global_metrics = eval_data.get('global_metrics', {})
        
        print(f"\nüìä CALIDAD DE GENERACI√ìN:")
        if 'diversity' in global_metrics:
            ttr = global_metrics['diversity'].get('type_token_ratio', 0)
            print(f"  ‚Ä¢ Diversidad (TTR): {ttr:.3f}")
        
        if 'coherence' in global_metrics:
            consistency = global_metrics['coherence'].get('consistency_score', 0)
            fluency = global_metrics['coherence'].get('fluency_score', 0)
            print(f"  ‚Ä¢ Consistencia: {consistency:.3f}")
            print(f"  ‚Ä¢ Fluidez: {fluency:.3f}")
    
    print(f"\nüéñÔ∏è  PR√ìXIMOS PASOS:")
    print(f"  1. Analizar resultados en dashboard (puerto 8501/8502)")
    print(f"  2. Considerar entrenar modelos con configuraciones optimizadas")
    print(f"  3. Implementar mejoras sugeridas en generaci√≥n de texto")
    print(f"  4. Explorar arquitecturas avanzadas (DynamicMemory, HierarchicalAttention)")

def main():
    """Funci√≥n principal del an√°lisis."""
    print(f"üî¨ INICIANDO AN√ÅLISIS COMPLETO DEL ENTRENAMIENTO RECIENTE")
    print(f"üïí {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # Ejecutar an√°lisis
        analyze_model_performance()
        analyze_training_evolution()
        generate_recommendations()
        create_summary_report()
        
        print(f"\n{'='*70}")
        print(f"‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE")
        print(f"{'='*70}")
        
    except Exception as e:
        print(f"\n‚ùå Error durante el an√°lisis: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()