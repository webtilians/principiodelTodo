#!/usr/bin/env python3
"""
üîÆ INFINITO JARVIS - CONECTADO A OPENAI
========================================

Sistema completo que combina:
1. Tu modelo Infinito (Gate Din√°mico) para filtrar qu√© recordar
2. OpenAI GPT para generar respuestas inteligentes
3. Memoria persistente en JSON

¬°Ahora con respuestas REALES de GPT!
"""

import torch
import torch.nn as nn
import json
import os
import sys
from datetime import datetime
from openai import OpenAI

# A√±adir src al path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))
from infinito_v5_2_refactored import InfinitoV52Refactored

# --- CONFIGURACI√ìN OPENAI ---
# Configura tu API key como variable de entorno: set OPENAI_API_KEY=tu-api-key
API_KEY = os.environ.get("OPENAI_API_KEY", "")

if API_KEY.startswith("sk-") and len(API_KEY) > 10:
    client = OpenAI(api_key=API_KEY)
    print("‚úÖ Conexi√≥n con OpenAI configurada.")
else:
    print("‚ö†Ô∏è API Key no configurada. Usa: set OPENAI_API_KEY=tu-api-key")
    client = None


# --- MODELO INFINITO (Gate Din√°mico) ---
class InfinitoDynamicChat(InfinitoV52Refactored):
    """Modelo con gate din√°mico para detectar informaci√≥n importante."""
    
    def __init__(self, *args, **kwargs):
        kwargs['use_dynamic_gate'] = False
        super().__init__(*args, **kwargs)
        
        if hasattr(self, 'memory_gate'):
            del self.memory_gate
        
        self.gate_network = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.GELU(),
            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),
            nn.GELU(),
            nn.Linear(self.hidden_dim // 4, 1)
        )
    
    def forward(self, input_ids, return_metrics=False):
        batch_size, seq_len = input_ids.shape
        
        hidden = self.token_embedding(input_ids)
        hidden = hidden + self.position_embedding[:, :seq_len, :]
        hidden = self.embedding_dropout(hidden)
        
        for attn, ff, ln1, ln2 in zip(
            self.attention_layers, self.ff_layers, 
            self.layer_norms_1, self.layer_norms_2
        ):
            attn_out, _ = attn(hidden)
            hidden = ln1(hidden + attn_out)
            ff_out = ff(hidden)
            hidden = ln2(hidden + ff_out)
        
        sentence_context = hidden.mean(dim=1)
        gate_logit = self.gate_network(sentence_context)
        gate_open_pct = torch.sigmoid(gate_logit)
        
        logits = self.output_projection(hidden)
        
        if return_metrics:
            return logits, {'gate_value': gate_open_pct.mean().item()}
        return logits, None


def text_to_ids(text, seq_len=32):
    """Convierte texto a IDs ASCII."""
    ids = [ord(c) % 256 for c in text]
    if len(ids) < seq_len:
        ids = ids + [0] * (seq_len - len(ids))
    else:
        ids = ids[:seq_len]
    return torch.tensor([ids])


# --- SISTEMA JARVIS CON OPENAI ---
class JarvisSystem:
    """Asistente inteligente con memoria selectiva y GPT."""
    
    def __init__(self, keeper_model_path, db_file="memoria_infinito.json"):
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.db_file = db_file
        
        print(f"\n{'='*60}")
        print(f"üîÆ INFINITO JARVIS + OPENAI")
        print(f"{'='*60}")
        print(f"Device: {self.device}")
        
        # 1. Cargar modelo Infinito (el "portero" de la memoria)
        self._load_keeper(keeper_model_path)
        
        # 2. Cargar memorias existentes
        self._load_memories()
        
        print(f"{'='*60}\n")
    
    def _load_keeper(self, model_path):
        """Carga el modelo Infinito."""
        print(f"üß† Cargando Infinito Gate...")
        
        self.keeper = InfinitoDynamicChat(
            vocab_size=256,
            hidden_dim=64,
            num_layers=2,
            num_heads=4,
            use_improved_memory=True,
            use_improved_iit=True,
        ).to(self.device)
        
        try:
            checkpoint = torch.load(model_path, weights_only=False, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                self.keeper.load_state_dict(checkpoint['model_state_dict'], strict=False)
            else:
                self.keeper.load_state_dict(checkpoint, strict=False)
            self.keeper.eval()
            print("   ‚úÖ Modelo cargado")
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            sys.exit(1)
    
    def _load_memories(self):
        """Carga las memorias del archivo JSON."""
        if os.path.exists(self.db_file):
            with open(self.db_file, 'r', encoding='utf-8') as f:
                self.memories = json.load(f)
            print(f"üìö {len(self.memories)} recuerdos cargados")
        else:
            self.memories = []
            print("‚ú® Nueva memoria iniciada")
    
    def _es_pregunta(self, texto):
        """Detecta si el usuario est√° preguntando."""
        t = texto.lower().strip()
        if '?' in t or '¬ø' in t:
            return True
        starts = ['que ', 'qu√© ', 'como ', 'c√≥mo ', 'cual ', 'cu√°l ', 'quien ', 'qui√©n ', 
                  'donde ', 'd√≥nde ', 'cuando ', 'cu√°ndo ', 'cuanto ', 'cu√°nto ', 
                  'por que ', 'por qu√© ', 'y como ', 'y quien ', 'sabes ', 'recuerdas ']
        for s in starts:
            if t.startswith(s):
                return True
        return False
    
    def _analyze_importance(self, text):
        """Usa el modelo Infinito para medir importancia."""
        inp = text_to_ids(text).to(self.device)
        with torch.no_grad():
            _, metrics = self.keeper(inp, return_metrics=True)
        return metrics['gate_value'] * 100
    
    def _categorize(self, text):
        """Categoriza el tipo de informaci√≥n."""
        text_lower = text.lower()
        
        if any(x in text_lower for x in ['me llamo', 'mi nombre es', 'soy ', 'll√°mame']):
            return "üë§ identidad"
        elif any(x in text_lower for x in ['contrase√±a', 'clave', 'password', 'pin', 'secreto']):
            return "üîê credencial"
        elif any(x in text_lower for x in ['tel√©fono', 'email', 'correo', 'direcci√≥n', 'vivo en']):
            return "üìû contacto"
        elif any(x in text_lower for x in ['recuerda', 'no olvides', 'importante', 'ma√±ana', 'cita']):
            return "üìå recordatorio"
        elif any(x in text_lower for x in ['me gusta', 'prefiero', 'favorito', 'odio']):
            return "‚ù§Ô∏è preferencia"
        elif any(x in text_lower for x in ['mi primo', 'mi hermano', 'mi madre', 'mi padre', 'mi amigo']):
            return "üë®‚Äçüë©‚Äçüëß familia"
        else:
            return "üìù general"
    
    def _save_memory(self, text, score, category):
        """Guarda un recuerdo importante."""
        entry = {
            "id": len(self.memories) + 1,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "content": text,
            "score": round(score, 1),
            "category": category
        }
        self.memories.append(entry)
        
        with open(self.db_file, 'w', encoding='utf-8') as f:
            json.dump(self.memories, f, indent=2, ensure_ascii=False)
    
    def _construct_prompt(self):
        """Crea el prompt del sistema con la memoria."""
        memory_block = "NO TIENES RECUERDOS PREVIOS DEL USUARIO."
        
        if self.memories:
            memory_block = "üìö MEMORIA A LARGO PLAZO (Hechos que conoces sobre el usuario):\n"
            for mem in self.memories[-20:]:  # √öltimos 20 recuerdos
                memory_block += f"  ‚Ä¢ {mem['content']} [{mem.get('category', 'general')}] (Guardado: {mem['timestamp']})\n"
        
        system_prompt = f"""Eres Infinito, un asistente personal avanzado con MEMORIA PERSISTENTE.

{memory_block}

INSTRUCCIONES CR√çTICAS:
1. USA la memoria anterior para personalizar TODAS tus respuestas
2. Si el usuario pregunta algo que est√° en tu memoria (nombres, claves, datos), RESPONDE DIRECTAMENTE con esa informaci√≥n
3. Si te preguntan "¬øc√≥mo me llamo?" y tienes su nombre en memoria, DILO
4. Si te preguntan sobre familiares/amigos y tienes esa info, √öSALA
5. S√© breve, √∫til y amable
6. NUNCA digas "no tengo acceso a esa informaci√≥n" si est√° en tu memoria
7. Responde en espa√±ol

Eres como Jarvis de Iron Man, pero con memoria real sobre tu usuario."""
        
        return system_prompt
    
    def chat(self, user_text):
        """Procesa un mensaje del usuario."""
        
        # --- FASE 1: INFINITO KEEPER (El Filtro) ---
        importance = self._analyze_importance(user_text)
        is_question = self._es_pregunta(user_text)
        
        # L√≥gica de decisi√≥n
        should_save = (importance > 50.0) and (not is_question)
        
        # Feedback visual
        bar_len = int(importance / 5)
        bar = "‚ñà" * bar_len + "‚ñë" * (20 - bar_len)
        
        print(f"\n   ‚îå{'‚îÄ'*48}‚îê")
        print(f"   ‚îÇ üîç Gate: [{bar}] {importance:>6.1f}% ", end="")
        
        if should_save:
            category = self._categorize(user_text)
            self._save_memory(user_text, importance, category)
            print(f"üü¢ ‚îÇ")
            print(f"   ‚îÇ üíæ Guardado: {category:<30} ‚îÇ")
        elif is_question and importance > 50.0:
            print(f"üü° ‚îÇ")
            print(f"   ‚îÇ ‚ùì Pregunta detectada (consultando memoria)  ‚îÇ")
        else:
            print(f"üî¥ ‚îÇ")
        print(f"   ‚îî{'‚îÄ'*48}‚îò")
        
        # --- FASE 2: OPENAI GPT (El Orador) ---
        try:
            prompt_system = self._construct_prompt()
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",  # Puedes cambiar a "gpt-4" si prefieres
                messages=[
                    {"role": "system", "content": prompt_system},
                    {"role": "user", "content": user_text}
                ],
                temperature=0.7,
                max_tokens=500
            )
            return response.choices[0].message.content
            
        except Exception as e:
            return f"‚ùå Error de OpenAI: {e}"
    
    def show_memory(self):
        """Muestra la memoria completa."""
        print(f"\n{'='*60}")
        print(f"üß† MEMORIA DE INFINITO ({len(self.memories)} recuerdos)")
        print(f"{'='*60}")
        
        if not self.memories:
            print("   (La memoria est√° vac√≠a)")
        else:
            for mem in self.memories:
                cat = mem.get('category', 'üìù general')
                print(f"\n   {cat} #{mem['id']} [{mem['timestamp']}]")
                print(f"      \"{mem['content']}\"")
                print(f"      Importancia: {mem['score']}%")
        
        print(f"\n{'='*60}")


def main():
    """Funci√≥n principal."""
    
    MODEL_PATH = os.path.join(os.path.dirname(__file__), "models", "dynamic_chat_detector_v2.pt")
    
    if not os.path.exists(MODEL_PATH):
        print(f"‚ùå No se encontr√≥ el modelo en: {MODEL_PATH}")
        print("   Ejecuta primero: python test_dynamic_chat_v2.py")
        sys.exit(1)
    
    # Inicializar Jarvis
    jarvis = JarvisSystem(MODEL_PATH)
    
    # Instrucciones
    print("üí¨ Chatea conmigo. Ahora con GPT REAL.")
    print("   Escribe 'ver memoria' para ver recuerdos.")
    print("   Escribe 'salir' para terminar.")
    print("‚îÄ" * 60)
    
    while True:
        try:
            user_input = input("\nüë§ T√∫ > ").strip()
            
            if not user_input:
                continue
            
            cmd = user_input.lower()
            
            if cmd in ['salir', 'exit', 'quit']:
                break
            elif cmd == 'ver memoria':
                jarvis.show_memory()
                continue
            elif cmd == 'borrar memoria':
                confirm = input("   ¬øBorrar toda la memoria? (s/n): ")
                if confirm.lower() == 's':
                    jarvis.memories = []
                    with open(jarvis.db_file, 'w') as f:
                        json.dump([], f)
                    print("   üóëÔ∏è Memoria borrada")
                continue
            
            # Chat con GPT
            print("   ü§î Pensando...", end="\r")
            response = jarvis.chat(user_input)
            print(f"\nü§ñ Infinito > {response}")
            
        except KeyboardInterrupt:
            print("\n")
            break
        except EOFError:
            break
    
    print(f"\n{'='*60}")
    print(f"üëã ¬°Hasta luego!")
    print(f"   Recuerdos guardados: {len(jarvis.memories)}")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
