# üéÆ Sistema RL para INFINITO - Scheduler Œ¶ vs Texto

## üìñ Descripci√≥n

Sistema de **Aprendizaje por Refuerzo (RL)** que controla din√°micamente el balance entre optimizaci√≥n de **texto** y **PHI (Œ¶)** en INFINITO.

Un agente **PPO** (Proximal Policy Optimization) aprende a decidir cu√°ndo priorizar:
- **Modo TEXTO**: Optimizar calidad del lenguaje (w_text=1.0, w_phi=0.0)
- **Modo PHI**: Optimizar integraci√≥n de informaci√≥n (w_text=0.1, w_phi=1.0)
- **Modo MIXTO**: Balance equilibrado (w_text=0.5, w_phi=0.5)

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Agente RL (PPO)                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Observa: [C, Œ¶, loss_text, loss_phi, ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ           memory_util, time]           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                    ‚Üì                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Decide: acci√≥n ‚àà {TEXT, PHI, MIXED}  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         INFINITO (InfinitoGPT2Hybrid)       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Ajusta pesos: w_text, w_phi          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Loss = w_text¬∑loss_LM + w_phi¬∑loss_Œ¶ ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                    ‚Üì                         ‚îÇ
‚îÇ  Entrena N pasos con configuraci√≥n actual   ‚îÇ
‚îÇ                    ‚Üì                         ‚îÇ
‚îÇ  Devuelve: m√©tricas actualizadas            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
          r = Œ±¬∑ŒîC + Œ≤¬∑ŒîŒ¶ + Œ≥¬∑Œîppl - Œ¥¬∑cost
```

## üéØ Recompensa (Versi√≥n Mejorada v2)

El agente maximiza una recompensa compuesta **mejorada** con t√©rminos adicionales:

```
r = Œ±¬∑ŒîC + Œ≤¬∑ŒîŒ¶ + Œ≥¬∑Œîperplexity - Œ¥¬∑cost + estabilidad + balances
```

### T√©rminos B√°sicos
- **Œ±¬∑ŒîC**: Mejora en "consciousness" (PHI normalizado)
- **Œ≤¬∑ŒîŒ¶**: Mejora en PHI absoluto
- **Œ≥¬∑Œîperplexity**: Mejora en perplexity (negativo si empeora)
- **Œ¥¬∑cost**: Penalizaci√≥n por uso de memoria (proxy de compute)

### T√©rminos Mejorados (NUEVO)
- **Estabilidad PHI**: Penaliza cambios bruscos (|ŒîŒ¶| > 1.0) ‚Üí -0.5¬∑(|ŒîŒ¶| - 1.0)
- **Balance PHI**: Mantiene Œ¶ ‚àà [3.0, 6.0] √≥ptimo
  - Œ¶ < 3.0: penaliza -0.3¬∑(3.0 - Œ¶)
  - Œ¶ > 6.0: penaliza **fuerte** -0.6¬∑(Œ¶ - 6.0) ‚Üê Evita colapso Fase 2
  - Œ¶ ‚àà [3.0, 6.0]: bonus +0.1
- **L√≠mites Perplexity**: Detecta colapso/confusi√≥n
  - PPL < 10: colapso ‚Üí -1.0¬∑(10 - PPL)/10
  - PPL > 200: confuso ‚Üí -0.3¬∑(PPL - 200)/100
- **Balance Consciousness**: C ‚àà [0.3, 0.7] √≥ptimo ‚Üí bonus +0.05

**Pesos por defecto**: `Œ±=1.0, Œ≤=0.5, Œ≥=0.1, Œ¥=0.2`

**Objetivo**: Evitar el colapso de Fase 2 (Œ¶ > 8 + repeticiones) incentivando rangos √≥ptimos.

## üì¶ Instalaci√≥n

### Dependencias

```bash
pip install gymnasium>=0.29.0
pip install stable-baselines3>=2.0.0
pip install tensorboard>=2.13.0
```

O instalar todas las dependencias del proyecto:

```bash
pip install -r requirements.txt
```

## üöÄ Uso

### 1. Entrenar Agente RL

Entrenar un agente PPO durante 100K timesteps:

```bash
python experiments/train_phi_text_scheduler.py \
    --timesteps 100000 \
    --inner-steps 5 \
    --max-steps 100 \
    --batch-size 4 \
    --lr 3e-4 \
    --output-dir outputs/rl_phi_text_scheduler
```

**Par√°metros**:
- `--timesteps`: Total de timesteps de entrenamiento (default: 100,000)
- `--inner-steps`: Pasos de INFINITO por step RL (default: 5)
- `--max-steps`: Pasos m√°ximos por episodio RL (default: 100)
- `--batch-size`: Batch size para INFINITO (default: 4)
- `--lr`: Learning rate del agente PPO (default: 3e-4)
- `--output-dir`: Directorio de salida
- `--save-freq`: Frecuencia de checkpoints (default: 10,000)
- `--eval-freq`: Frecuencia de evaluaci√≥n (default: 5,000)

### 2. Monitorear Entrenamiento

Ver logs con TensorBoard:

```bash
tensorboard --logdir outputs/rl_phi_text_scheduler/tensorboard
```

M√©tricas disponibles:
- `rollout/ep_rew_mean`: Recompensa promedio
- `rollout/ep_len_mean`: Longitud promedio de episodios
- `train/policy_loss`: Loss de la pol√≠tica
- `train/value_loss`: Loss de la funci√≥n de valor
- `train/entropy_loss`: Entrop√≠a de la pol√≠tica

### 3. Ejecutar Demostraci√≥n

Ejecutar INFINITO controlado por el agente entrenado:

```bash
python experiments/run_infinito_with_scheduler.py \
    --model outputs/rl_phi_text_scheduler/ppo_infinito_scheduler_final.zip \
    --episodes 3 \
    --max-steps 100 \
    --output-dir outputs/rl_demo
```

Esto generar√°:
- Logs de ejecuci√≥n en consola
- Gr√°ficos de m√©tricas por episodio en `outputs/rl_demo/`

### 4. Usar en Script Personalizado

```python
from stable_baselines3 import PPO
from src.rl.infinito_rl_env import InfinitoRLEnv

# Configurar entorno
env_config = {
    "inner_steps": 5,
    "max_steps": 100,
    "model_kwargs": {
        "use_lora": True,
        "lambda_phi": 0.3,
    },
}

# Crear entorno
env = InfinitoRLEnv(config=env_config)

# Cargar agente entrenado
model = PPO.load("path/to/model.zip")

# Ejecutar
obs = env.reset()
done = False

while not done:
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
    
    # Modo actual
    mode = ["TEXT", "PHI", "MIXED"][action]
    print(f"Modo: {mode}, Reward: {reward:.4f}")
```

## üìä Resultados Esperados

### Entrenamiento

Durante el entrenamiento, el agente deber√≠a:

1. **Fase Inicial (0-20K timesteps)**:
   - Exploraci√≥n aleatoria
   - Recompensas fluctuantes
   - No hay patr√≥n claro

2. **Fase de Aprendizaje (20K-60K timesteps)**:
   - Recompensas comienzan a subir
   - Aparecen patrones en acciones
   - Agente aprende qu√© funciona

3. **Fase de Convergencia (60K-100K timesteps)**:
   - Recompensas estables
   - Pol√≠tica consistente
   - Balance adaptativo emergente

### Comportamiento del Agente

El agente deber√≠a aprender patrones como:

- **Inicio de episodio**: Preferir modo **TEXTO** para estabilizar
- **PHI bajo**: Cambiar a modo **PHI** para aumentar integraci√≥n
- **PHI alto pero perplexity alta**: Modo **MIXTO** para balancear
- **Cerca de breakthrough (C > 0.6)**: Mantener modo actual

## üî¨ Componentes

### InfinitoRLEnv

Entorno Gymnasium para RL:

```python
class InfinitoRLEnv(gym.Env):
    """
    Espacio de acciones: Discrete(3)
      0 ‚Üí Modo TEXTO
      1 ‚Üí Modo PHI
      2 ‚Üí Modo MIXTO
    
    Espacio de observaciones: Box(6)
      [C, Œ¶, loss_text, loss_phi, memory_util, time_norm]
    """
```

**M√©todos clave**:
- `reset()`: Reinicia INFINITO para nuevo episodio
- `step(action)`: Ejecuta acci√≥n y devuelve (obs, reward, done, info)
- `_compute_reward()`: Calcula recompensa basada en mejoras

### InfinitoGPT2Hybrid (Modificado)

Modelo INFINITO con soporte RL:

**Nuevos m√©todos**:
```python
model.set_loss_weights(w_text, w_phi)  # Cambiar pesos din√°micamente
model.get_current_metrics()             # Obtener m√©tricas actuales
model.update_current_metrics(...)       # Actualizar m√©tricas internas
```

**Atributo**:
```python
model.loss_weights = {"text": 1.0, "phi": 0.3}  # Pesos actuales
```

## üìà Hiperpar√°metros

### PPO

- **Policy**: MLP (Multi-Layer Perceptron)
- **Hidden layers**: [128, 128] (actor), [128, 128] (critic)
- **Learning rate**: 3e-4
- **Discount (Œ≥)**: 0.99
- **GAE (Œª)**: 0.95
- **PPO clip**: 0.2
- **Entropy coef**: 0.01
- **Value function coef**: 0.5

### INFINITO (para RL)

- **LoRA r**: 4 (reducido para velocidad)
- **LoRA alpha**: 16
- **Memory slots**: 128 (reducido para velocidad)
- **Sequence length**: 128 (reducido para velocidad)
- **Batch size**: 4 (peque√±o para RL)

## üêõ Troubleshooting

### Error: "stable-baselines3 not found"

```bash
pip install stable-baselines3
```

### Error: "gymnasium not found"

```bash
pip install gymnasium
```

O si usas gym antiguo:

```bash
pip install gym
```

### CUDA Out of Memory

Reducir:
- `batch_size` en env_config
- `inner_steps` (menos pasos por step RL)
- `lora_r` en model_kwargs

### Entrenamiento muy lento

Reducir:
- `max_steps` por episodio
- `inner_steps` por step
- `total_timesteps` de entrenamiento

## üìù Notas

1. **Tiempo de entrenamiento**: ~2-4 horas para 100K timesteps (GPU)
2. **Memoria GPU**: ~6-8GB con configuraci√≥n por defecto
3. **Checkpoints**: Guardados cada 10K timesteps en `output_dir/checkpoints/`
4. **Mejor modelo**: Guardado en `output_dir/best_model/` seg√∫n evaluaci√≥n

## üîÆ Experimentos Sugeridos

1. **Pesos de recompensa**: Probar diferentes Œ±, Œ≤, Œ≥, Œ¥
2. **Inner steps**: Variar cantidad de pasos INFINITO por step RL
3. **Arquitectura PPO**: Probar redes m√°s grandes/peque√±as
4. **Curriculum learning**: Empezar con episodios cortos, aumentar gradualmente
5. **Multi-objetivo**: A√±adir m√°s componentes a la recompensa

## üìö Referencias

- [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/)
- [Gymnasium Docs](https://gymnasium.farama.org/)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [IIT Theory](http://integratedinformationtheory.org/)

---

**√öltima actualizaci√≥n**: 11 de Noviembre, 2025  
**Versi√≥n**: 1.0.0 - Sistema RL Scheduler Œ¶ vs Texto
