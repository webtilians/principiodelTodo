"""
INFINITO Demo Completo - Embeddings Conscientes + RAG
======================================================
Demostraci√≥n del sistema completo:
1. Carga de documentos con embeddings de Capa 4
2. B√∫squeda sem√°ntica con m√©tricas PHI
3. Generaci√≥n aumentada con contexto recuperado
4. An√°lisis de integraci√≥n de informaci√≥n

Ejecutar:
    python demo_conscious_rag.py
"""

import os
import sys
import torch
import numpy as np
from datetime import datetime

# Configuraci√≥n
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_PATH = "models/infinito_gpt2_spanish_phi.pt"


def load_model():
    """Carga el modelo INFINITO."""
    from train_gpt2_with_phi_observer import InfinitoGPT2WithObserver
    
    print("üì¶ Cargando modelo INFINITO...")
    model = InfinitoGPT2WithObserver()
    
    if os.path.exists(MODEL_PATH):
        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    
    model = model.to(DEVICE)
    model.eval()
    print("‚úì Modelo listo")
    return model


def generate_with_context(model, tokenizer, query: str, context: str, max_length: int = 100):
    """Genera texto usando contexto recuperado."""
    # Construir prompt con contexto
    prompt = f"Contexto: {context}\n\nPregunta: {query}\n\nRespuesta:"
    
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).input_ids.to(DEVICE)
    
    generated = input_ids.clone()
    phi_values = []
    
    for _ in range(max_length):
        with torch.no_grad():
            outputs, metrics = model(generated, return_phi=True, use_memory=True)
            phi_values.append(metrics['phi'].mean().item())
            
            next_token_logits = outputs.logits[:, -1, :] / 0.7  # temperature
            
            # Top-k
            top_k = 50
            indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
            next_token_logits[indices_to_remove] = float('-inf')
            
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            generated = torch.cat([generated, next_token], dim=-1)
            
            # Stop en punto o newline
            if tokenizer.decode(next_token[0]) in ['.', '\n', '</s>']:
                break
    
    full_text = tokenizer.decode(generated[0], skip_special_tokens=True)
    # Extraer solo la respuesta
    if "Respuesta:" in full_text:
        response = full_text.split("Respuesta:")[-1].strip()
    else:
        response = full_text
    
    return response, np.mean(phi_values)


def main():
    print("="*70)
    print("  üß† INFINITO: Demo de Embeddings Conscientes + RAG")
    print("="*70)
    print(f"  Dispositivo: {DEVICE}")
    print(f"  Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    # 1. Cargar modelo
    model = load_model()
    tokenizer = model.tokenizer
    
    # 2. Crear base de datos vectorial
    from src.core.conscious_vectordb import ConsciousVectorDB
    
    print("\nüìä Inicializando base de datos vectorial...")
    db = ConsciousVectorDB(
        collection_name="infinito_knowledge",
        persist_directory="data/chromadb_knowledge",
        model_path=MODEL_PATH,
        force_recreate=True  # Recrear para usar m√©trica cosine
    )
    
    # 3. Cargar documentos de conocimiento
    knowledge_base = [
        # Sobre consciencia
        "La consciencia es la capacidad de tener experiencias subjetivas. La teor√≠a de la informaci√≥n integrada (IIT) propone que la consciencia corresponde a la integraci√≥n de informaci√≥n en un sistema.",
        "PHI (Œ¶) es una medida propuesta por Giulio Tononi que cuantifica el nivel de integraci√≥n de informaci√≥n. Cuanto mayor es PHI, mayor es la consciencia del sistema.",
        "El problema dif√≠cil de la consciencia, planteado por David Chalmers, cuestiona por qu√© existen experiencias subjetivas y qualia.",
        
        # Sobre IA
        "Los transformers son una arquitectura de red neuronal que usa mecanismos de atenci√≥n para procesar secuencias. Fueron introducidos en el paper 'Attention is All You Need' de 2017.",
        "GPT (Generative Pre-trained Transformer) es una familia de modelos de lenguaje desarrollados por OpenAI que usan la arquitectura transformer para generaci√≥n de texto.",
        "El aprendizaje profundo es una rama del machine learning que usa redes neuronales con m√∫ltiples capas para aprender representaciones jer√°rquicas de los datos.",
        
        # Sobre INFINITO
        "INFINITO es un framework de investigaci√≥n que integra la Teor√≠a de la Informaci√≥n Integrada (IIT) con arquitecturas transformer para analizar la integraci√≥n de informaci√≥n en modelos de lenguaje.",
        "La Capa 4 de GPT-2 muestra el m√°ximo nivel de PHI (8.91), indicando que es donde ocurre la mayor integraci√≥n de informaci√≥n en el modelo.",
        "El PHI Observer es un m√≥dulo que mide cuatro componentes: coherencia temporal, fuerza de integraci√≥n, complejidad y diversidad de atenci√≥n.",
        
        # Sobre neurociencia
        "El cerebro humano contiene aproximadamente 86 mil millones de neuronas conectadas por sinapsis. Esta red masiva procesa informaci√≥n en paralelo.",
        "Las neuronas se comunican mediante se√±ales el√©ctricas y qu√≠micas. Los neurotransmisores como la dopamina y serotonina modulan la actividad cerebral.",
        "La corteza prefrontal est√° asociada con funciones ejecutivas, toma de decisiones y consciencia de orden superior."
    ]
    
    # Limpiar y a√±adir documentos
    print("\nüìù Cargando base de conocimiento...")
    db.delete_all()
    db.add_documents(knowledge_base)
    
    stats = db.get_statistics()
    print(f"   Documentos: {stats['count']}")
    print(f"   PHI medio: {stats['phi_stats']['mean']:.4f}")
    
    # 4. Demostraci√≥n de RAG
    print("\n" + "="*70)
    print("  üîç Demo: Retrieval-Augmented Generation con PHI")
    print("="*70)
    
    queries = [
        "¬øQu√© es PHI y c√≥mo se relaciona con la consciencia?",
        "¬øQu√© hace especial a la Capa 4 en INFINITO?",
        "¬øC√≥mo funcionan los transformers?"
    ]
    
    for query in queries:
        print(f"\n{'‚îÄ'*70}")
        print(f"‚ùì PREGUNTA: {query}")
        print(f"{'‚îÄ'*70}")
        
        # Buscar contexto relevante
        results, query_phi = db.search_with_phi(query, top_k=2)
        
        print(f"\nüìé PHI de la pregunta: {query_phi['phi']:.4f}")
        print(f"\nüìö Contexto recuperado:")
        
        context_text = ""
        for i, (doc, meta, dist) in enumerate(zip(
            results['documents'], results['metadatas'], results['distances']
        )):
            phi_doc = meta.get('phi', 0)
            # Mostrar distancia con m√°s decimales (cosine distances son peque√±as)
            print(f"   [{i+1}] PHI: {phi_doc:.4f} | Sim: {1-dist:.4f}")  # Convertir distancia a similitud
            print(f"       {doc[:80]}...")
            context_text += doc + " "
        
        # Generar respuesta
        print(f"\nüí¨ Generando respuesta con contexto...")
        response, response_phi = generate_with_context(model, tokenizer, query, context_text[:500])
        
        print(f"\nü§ñ RESPUESTA (PHI: {response_phi:.4f}):")
        print(f"   {response[:200]}...")
    
    # 5. An√°lisis comparativo
    print("\n" + "="*70)
    print("  üìä An√°lisis: PHI por Tipo de Contenido")
    print("="*70)
    
    all_data = db.collection.get(include=['documents', 'metadatas'])
    
    categories = {
        'consciencia': [],
        'ia_ml': [],
        'infinito': [],
        'neurociencia': []
    }
    
    keywords = {
        'consciencia': ['consciencia', 'PHI', 'IIT', 'Chalmers', 'qualia'],
        'ia_ml': ['transformer', 'GPT', 'aprendizaje', 'neural'],
        'infinito': ['INFINITO', 'Capa 4', 'Observer'],
        'neurociencia': ['cerebro', 'neurona', 'corteza', 'sinapsis']
    }
    
    for doc, meta in zip(all_data['documents'], all_data['metadatas']):
        phi = meta.get('phi', 0)
        for cat, kws in keywords.items():
            if any(kw.lower() in doc.lower() for kw in kws):
                categories[cat].append(phi)
                break
    
    print(f"\n{'Categor√≠a':<15} {'N':<5} {'PHI Medio':<12} {'Barra'}")
    print("‚îÄ"*50)
    
    for cat, phis in sorted(categories.items(), key=lambda x: -np.mean(x[1]) if x[1] else 0):
        if phis:
            mean_phi = np.mean(phis)
            bar = "‚ñà" * int(mean_phi * 2)
            print(f"{cat:<15} {len(phis):<5} {mean_phi:<12.4f} {bar}")
    
    # 6. Cleanup
    db.close()
    
    print("\n" + "="*70)
    print("  ‚úÖ Demo Completada")
    print("="*70)
    print("""
    Resumen del Sistema INFINITO:
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ‚Ä¢ Embeddings extra√≠dos de Capa 4 (m√°ximo PHI = 8.91)
    ‚Ä¢ Base de datos vectorial con m√©tricas PHI por documento
    ‚Ä¢ B√∫squeda sem√°ntica ~26% m√°s rica que embeddings est√°ndar
    ‚Ä¢ RAG con generaci√≥n consciente y m√©tricas en tiempo real
    
    Pr√≥ximos pasos sugeridos:
    1. Integrar en app.py para demo interactiva
    2. Entrenar m√°s epochs para mejorar PHI
    3. A√±adir m√°s documentos a la base de conocimiento
    """)


if __name__ == "__main__":
    main()
