#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸ§ª TEST - Nueva Reward Function v2
===================================

Script para verificar que la nueva reward function con tÃ©rminos
mejorados funciona correctamente.
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

from src.rl.infinito_rl_env import InfinitoRLEnv


def test_reward_scenarios():
    """Probar la reward function en escenarios especÃ­ficos."""
    
    print("="*70)
    print("TEST - REWARD FUNCTION v2 (MEJORADA)")
    print("="*70)
    
    # Crear entorno con configuraciÃ³n mÃ­nima
    config = {
        "inner_steps": 1,
        "max_steps": 5,
        "batch_size": 2,
        "model_kwargs": {
            "use_lora": True,
            "lora_r": 4,
            "lora_alpha": 16,
            "lambda_phi": 0.3,
            "freeze_base": True,
            "memory_slots": 64,
        },
    }
    
    print("\nðŸ“¦ Creando entorno...")
    env = InfinitoRLEnv(config=config)
    print("âœ… Entorno creado")
    
    print("\n" + "="*70)
    print("ESCENARIOS DE PRUEBA")
    print("="*70)
    
    # Escenario 1: MÃ©tricas normales y estables
    print("\n1ï¸âƒ£ ESCENARIO: MÃ©tricas normales (PHI=4.5, C=0.5, PPL=80)")
    prev = {
        "consciousness": 0.48,
        "phi": 4.3,
        "loss_text": 4.5,
        "loss_phi": 1.2,
        "perplexity": 85.0,
        "memory_utilization": 0.3
    }
    cur = {
        "consciousness": 0.50,  # +0.02
        "phi": 4.5,             # +0.2 (estable)
        "loss_text": 4.4,
        "loss_phi": 1.1,
        "perplexity": 80.0,     # -5 (mejora)
        "memory_utilization": 0.3
    }
    reward = env._compute_reward(prev, cur)
    print(f"   Recompensa: {reward:.4f}")
    print(f"   Esperado: Positiva (mejoras pequeÃ±as)")
    print(f"   âœ… Balance Ã³ptimo" if reward > 0 else "   âš ï¸ Revisar")
    
    # Escenario 2: PHI en rango Ã³ptimo
    print("\n2ï¸âƒ£ ESCENARIO: PHI en rango Ã³ptimo [3.0-6.0]")
    prev = {
        "consciousness": 0.45,
        "phi": 4.8,
        "loss_text": 4.3,
        "loss_phi": 1.0,
        "perplexity": 90.0,
        "memory_utilization": 0.2
    }
    cur = {
        "consciousness": 0.47,
        "phi": 5.0,  # Dentro del rango Ã³ptimo
        "loss_text": 4.2,
        "loss_phi": 0.95,
        "perplexity": 85.0,
        "memory_utilization": 0.2
    }
    reward = env._compute_reward(prev, cur)
    print(f"   Recompensa: {reward:.4f}")
    print(f"   Esperado: Positiva con bonus por balance")
    print(f"   âœ… Incentivo correcto" if reward > 0 else "   âš ï¸ Revisar")
    
    # Escenario 3: PHI alto (peligro Fase 2)
    print("\n3ï¸âƒ£ ESCENARIO: PHI alto > 6.0 (riesgo colapso Fase 2)")
    prev = {
        "consciousness": 0.52,
        "phi": 6.5,
        "loss_text": 4.0,
        "loss_phi": 0.8,
        "perplexity": 70.0,
        "memory_utilization": 0.4
    }
    cur = {
        "consciousness": 0.54,
        "phi": 7.0,  # Â¡ALTO! Zona de colapso
        "loss_text": 3.9,
        "loss_phi": 0.7,
        "perplexity": 65.0,
        "memory_utilization": 0.4
    }
    reward = env._compute_reward(prev, cur)
    print(f"   Recompensa: {reward:.4f}")
    print(f"   Esperado: PENALIZACIÃ“N fuerte (-0.6 por cada unidad > 6.0)")
    print(f"   âœ… PenalizaciÃ³n aplicada" if reward < 0 else "   âŒ NO PENALIZA")
    
    # Escenario 4: Perplexity colapso
    print("\n4ï¸âƒ£ ESCENARIO: Perplexity < 10 (colapso/repeticiÃ³n)")
    prev = {
        "consciousness": 0.48,
        "phi": 5.2,
        "loss_text": 3.5,
        "loss_phi": 0.5,
        "perplexity": 12.0,
        "memory_utilization": 0.3
    }
    cur = {
        "consciousness": 0.49,
        "phi": 5.3,
        "loss_text": 3.3,
        "loss_phi": 0.45,
        "perplexity": 5.0,  # Â¡COLAPSO!
        "memory_utilization": 0.3
    }
    reward = env._compute_reward(prev, cur)
    print(f"   Recompensa: {reward:.4f}")
    print(f"   Esperado: PENALIZACIÃ“N fuerte (colapso detectado)")
    print(f"   âœ… Colapso detectado" if reward < -0.3 else "   âŒ NO DETECTA COLAPSO")
    
    # Escenario 5: Cambio brusco en PHI (inestabilidad)
    print("\n5ï¸âƒ£ ESCENARIO: Cambio brusco PHI (|Î”Î¦| > 1.0)")
    prev = {
        "consciousness": 0.45,
        "phi": 4.0,
        "loss_text": 4.5,
        "loss_phi": 1.2,
        "perplexity": 90.0,
        "memory_utilization": 0.3
    }
    cur = {
        "consciousness": 0.46,
        "phi": 6.5,  # Â¡+2.5! Cambio brusco
        "loss_text": 4.3,
        "loss_phi": 1.0,
        "perplexity": 85.0,
        "memory_utilization": 0.3
    }
    reward = env._compute_reward(prev, cur)
    print(f"   Recompensa: {reward:.4f}")
    print(f"   Esperado: PenalizaciÃ³n por inestabilidad")
    print(f"   âœ… Inestabilidad penalizada" if reward < 0 else "   âš ï¸ Revisar")
    
    # Escenario 6: PHI bajo (insuficiente)
    print("\n6ï¸âƒ£ ESCENARIO: PHI bajo < 3.0")
    prev = {
        "consciousness": 0.35,
        "phi": 3.2,
        "loss_text": 5.0,
        "loss_phi": 1.5,
        "perplexity": 120.0,
        "memory_utilization": 0.2
    }
    cur = {
        "consciousness": 0.36,
        "phi": 2.5,  # Â¡BAJO!
        "loss_text": 4.9,
        "loss_phi": 1.4,
        "perplexity": 115.0,
        "memory_utilization": 0.2
    }
    reward = env._compute_reward(prev, cur)
    print(f"   Recompensa: {reward:.4f}")
    print(f"   Esperado: PenalizaciÃ³n leve por PHI bajo")
    print(f"   âœ… PHI bajo penalizado" if reward < 0 else "   âš ï¸ Revisar")
    
    # Escenario 7: Perplexity muy alto (confusiÃ³n)
    print("\n7ï¸âƒ£ ESCENARIO: Perplexity > 200 (modelo confuso)")
    prev = {
        "consciousness": 0.40,
        "phi": 4.2,
        "loss_text": 5.5,
        "loss_phi": 1.8,
        "perplexity": 180.0,
        "memory_utilization": 0.3
    }
    cur = {
        "consciousness": 0.41,
        "phi": 4.3,
        "loss_text": 5.6,
        "loss_phi": 1.9,
        "perplexity": 250.0,  # Â¡MUY ALTO!
        "memory_utilization": 0.3
    }
    reward = env._compute_reward(prev, cur)
    print(f"   Recompensa: {reward:.4f}")
    print(f"   Esperado: PenalizaciÃ³n por confusiÃ³n")
    print(f"   âœ… ConfusiÃ³n penalizada" if reward < 0 else "   âš ï¸ Revisar")
    
    # Escenario 8: Estado Ã³ptimo perfecto
    print("\n8ï¸âƒ£ ESCENARIO: Estado Ã“PTIMO (C=0.5, PHI=4.5, PPL=75)")
    prev = {
        "consciousness": 0.48,
        "phi": 4.3,
        "loss_text": 4.5,
        "loss_phi": 1.1,
        "perplexity": 80.0,
        "memory_utilization": 0.25
    }
    cur = {
        "consciousness": 0.50,  # En rango [0.3, 0.7]
        "phi": 4.5,             # En rango [3.0, 6.0]
        "loss_text": 4.3,
        "loss_phi": 1.0,
        "perplexity": 75.0,     # En rango [10, 200]
        "memory_utilization": 0.25
    }
    reward = env._compute_reward(prev, cur)
    print(f"   Recompensa: {reward:.4f}")
    print(f"   Esperado: MÃXIMA POSITIVA (todo en rangos Ã³ptimos + bonuses)")
    print(f"   âœ… Estado Ã³ptimo recompensado" if reward > 0.1 else "   âš ï¸ Revisar bonuses")
    
    print("\n" + "="*70)
    print("âœ… TEST COMPLETADO")
    print("="*70)
    print("\nLa nueva reward function:")
    print("  âœ“ Incentiva PHI en rango [3.0, 6.0]")
    print("  âœ“ Penaliza fuerte PHI > 6.0 (evita Fase 2)")
    print("  âœ“ Detecta colapso por PPL < 10")
    print("  âœ“ Penaliza inestabilidad (cambios bruscos)")
    print("  âœ“ Recompensa estados Ã³ptimos con bonuses")
    
    env.close()


if __name__ == "__main__":
    try:
        test_reward_scenarios()
    except Exception as e:
        print(f"\nâŒ ERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
