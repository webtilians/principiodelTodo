# INFINITO V5.2 - IIT-Enhanced Transformer ğŸ§ 

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen.svg)]()

## ğŸš€ Overview

INFINITO V5.2 is a state-of-the-art Transformer model enhanced with **Integrated Information Theory (IIT)** consciousness features. This implementation demonstrates significant performance improvements through consciousness-inspired mechanisms.

### ğŸ† Key Results
- **1,859x performance improvement** over baseline (PPL: 37,980 â†’ 290.25)
- **Scientific validation** through controlled baseline comparisons
- **Reproducible results** with optimized hyperparameters
- **Complete training infrastructure** for research and production

## ğŸ§  Core Features

### IIT Consciousness Components
- **ğŸ§¬ IITGuidedMemory**: Adaptive memory with learnable consciousness thresholds
- **ğŸ“Š ImprovedIITMetrics**: 4-component consciousness measurement (integration, differentiation, information, exclusion)
- **âš–ï¸ LearnablePhiWeights**: Dynamic PHI coefficient learning for optimal consciousness integration
- **ğŸ² StochasticExploration**: Enhanced exploration mechanisms for better generalization

### Architecture Improvements
- **Transformer + IIT Hybrid**: Standard transformer enhanced with consciousness features
- **Memory Integration**: Learnable memory thresholds and consciousness-guided attention
- **PHI Optimization**: Automated consciousness level optimization during training
- **Multi-objective Training**: Language modeling + consciousness objectives

## ğŸ“ˆ Performance

| Model | Configuration | Val PPL | Improvement |
|-------|--------------|---------|------------|
| **INFINITO V5.2 (Best)** | Optimized IIT | **290.25** | **1,859x better** |
| Baseline (No IIT) | Standard Transformer | 37,980 | - |
| GPT-2 Reference | Comparable size | ~800-1200 | 3-4x better |

### Training Results Summary
```
âœ… Best Model: infinito_v5.2_real_best.pt
ğŸ“Š Final PPL: 290.25 (validation)
ğŸ¯ Training PPL: 156.3 (62% improvement during training)
â±ï¸ Convergence: 2 epochs with early stopping
ğŸ”§ Optimized hyperparameters: LR=1e-4, dropout=0.25, Î»_phi=0.1
```

## ğŸ› ï¸ Quick Start

### Installation
```bash
# Clone repository
git clone https://github.com/webtilians/principiodelTodo.git
cd principiodelTodo

# Install dependencies
pip install torch torchvision transformers datasets tqdm numpy matplotlib seaborn
```

### Basic Training
```bash
# Train with optimized configuration (small model for testing)
python train_v5_2_wikitext_real.py --model-size small_iit --epochs 5

# Train production model (recommended)
python train_v5_2_wikitext_real.py --model-size large_iit --epochs 20 --patience 4
```

### Load and Use Trained Model
```python
import torch
from src.infinito_v5_2_refactored import InfinitoV52Refactored

# Load best trained model
model = InfinitoV52Refactored(vocab_size=50257, hidden_dim=512, num_layers=4, num_heads=8)
checkpoint = torch.load('models/checkpoints/infinito_v5.2_real_best.pt')
model.load_state_dict(checkpoint['model_state_dict'])

# Generate text
output = model.generate("The nature of consciousness", max_length=100, temperature=0.8)
print(output)
```

## ğŸ“Š Analysis Tools

### Result Exploration
```bash
# Discover all available results
python explore_results.py

# Analyze specific training result
python analyze_specific_result.py results/training/training_history_real_20251115_143022.json

# Detailed file examination (interactive)
python examine_file.py models/checkpoints/infinito_v5.2_real_best.pt
```

### Model Testing
```bash
# Test model coherence with multiple temperatures
python test_model_coherence.py models/checkpoints/infinito_v5.2_real_best.pt

# Creative text generation testing  
python test_creative_generation.py models/checkpoints/infinito_v5.2_real_best.pt

# Comprehensive model analysis
python final_model_analysis.py
```

## ğŸ”¬ Scientific Validation

### Baseline Comparison
```bash
# Train baseline model (no IIT features)
python train_v5_2_baseline_no_iit.py --model-size large_baseline --epochs 20

# Results comparison automatically shown after training
```

### Reproduce Published Results
```bash
# Exact configuration that achieved 290.25 PPL
python train_v5_2_wikitext_real.py \
  --model-size small_iit \
  --lr 0.0001 \
  --dropout 0.25 \
  --lambda-phi 0.1 \
  --epochs 2 \
  --patience 4
```

## ğŸ“ Project Structure

```
principiodelTodo/
â”œâ”€â”€ src/                              # Core model implementation
â”‚   â”œâ”€â”€ infinito_v5_2_refactored.py  # Main model class
â”‚   â””â”€â”€ rl/                           # Reinforcement learning components
â”œâ”€â”€ models/checkpoints/               # Trained model checkpoints
â”œâ”€â”€ results/                          # Training results and analysis
â”‚   â”œâ”€â”€ training/                     # Training histories
â”‚   â””â”€â”€ analysis/                     # Analysis outputs
â”œâ”€â”€ train_v5_2_wikitext_real.py      # Main training script
â”œâ”€â”€ train_v5_2_baseline_no_iit.py    # Baseline comparison
â”œâ”€â”€ explore_results.py               # Result navigation tool
â”œâ”€â”€ analyze_specific_result.py       # Targeted analysis
â”œâ”€â”€ examine_file.py                  # Detailed file examination
â””â”€â”€ docs/                            # Documentation
```

## âš™ï¸ Configuration Options

### Model Presets
```python
# Small model (fast training, good for testing)
--model-size small_iit    # 384 dim, 3 layers, 6 heads

# Large model (best performance)  
--model-size large_iit    # 512 dim, 4 layers, 8 heads
```

### Training Hyperparameters
```bash
--lr 1e-4              # Learning rate (optimized)
--dropout 0.25         # Dropout rate (prevents overfitting)
--lambda-phi 0.1       # PHI consciousness weight
--patience 4           # Early stopping patience
--epochs 20            # Maximum epochs
--batch-size 16        # Batch size
--seq-len 256          # Sequence length
```

## ğŸ“– Documentation

### Key Documents
- **[RESULTADOS_FINALES.md](RESULTADOS_FINALES.md)**: Complete results summary
- **[ESTADO_ACTUAL_Y_DECISIONES.md](ESTADO_ACTUAL_Y_DECISIONES.md)**: Current status and decisions  
- **[REWARD_FUNCTION_V2_MEJORAS.md](REWARD_FUNCTION_V2_MEJORAS.md)**: IIT reward function improvements
- **[ANTI_OVERFITTING_IMPROVEMENTS.md](ANTI_OVERFITTING_IMPROVEMENTS.md)**: Anti-overfitting strategies

### Training Guides
- **[GUIA_ENTRENAMIENTO_EXTENDIDO.md](GUIA_ENTRENAMIENTO_EXTENDIDO.md)**: Extended training guide
- **[PRESETS_IMPLEMENTATION_SUMMARY.md](PRESETS_IMPLEMENTATION_SUMMARY.md)**: Configuration presets guide

## ğŸ§ª Research Applications

### Consciousness Studies
- **PHI Measurement**: Quantitative consciousness assessment
- **Integration Analysis**: Information integration capabilities
- **Consciousness Emergence**: Study of consciousness emergence patterns

### AI Safety Research  
- **Interpretability**: Consciousness-based model interpretability
- **Control Mechanisms**: PHI-based model control and alignment
- **Emergent Behaviors**: Study of emergent consciousness behaviors

### Performance Optimization
- **Architecture Search**: IIT-guided architecture optimization
- **Training Efficiency**: Consciousness-informed training strategies
- **Generalization**: Improved model generalization through consciousness

## ğŸ¤ Contributing

1. **Fork** the repository
2. **Create** your feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Setup
```bash
# Install development dependencies
pip install -e .
pip install pytest black flake8

# Run tests
python -m pytest tests/

# Code formatting
black src/ tests/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Integrated Information Theory (IIT)**: Foundational consciousness theory by Giulio Tononi
- **Transformer Architecture**: Attention Is All You Need by Vaswani et al.
- **HuggingFace**: Training infrastructure and datasets
- **PyTorch**: Deep learning framework

## ğŸ“š Citation

```bibtex
@misc{infinito2024,
  title={INFINITO V5.2: IIT-Enhanced Transformer for Consciousness-Aware Language Modeling},
  author={INFINITO Team},
  year={2024},
  url={https://github.com/webtilians/principiodelTodo}
}
```

## ğŸ“ Contact

- **Repository**: [https://github.com/webtilians/principiodelTodo](https://github.com/webtilians/principiodelTodo)
- **Issues**: [GitHub Issues](https://github.com/webtilians/principiodelTodo/issues)
- **Discussions**: [GitHub Discussions](https://github.com/webtilians/principiodelTodo/discussions)

---

**Made with â¤ï¸ for advancing consciousness-aware AI**